{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thushan97/CURE/blob/master/cure.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04b5Y2KlVvaJ",
        "outputId": "405ea282-8ce1-4689-fa9d-6e1cb527ab58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CURE'...\n",
            "remote: Enumerating objects: 1031, done.\u001b[K\n",
            "remote: Counting objects: 100% (359/359), done.\u001b[K\n",
            "remote: Compressing objects: 100% (316/316), done.\u001b[K\n",
            "remote: Total 1031 (delta 69), reused 289 (delta 29), pack-reused 672\u001b[K\n",
            "Receiving objects: 100% (1031/1031), 79.63 MiB | 7.35 MiB/s, done.\n",
            "Resolving deltas: 100% (81/81), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Thushan97/CURE.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/CURE/* /content/"
      ],
      "metadata": {
        "id": "Lf4LBkfjV9pW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pretrain model download\n",
        "!wget \"https://zenodo.org/record/7030145/files/models.tar.xz?download=1\" -c -O 'models.tar.xz'\n",
        "!mkdir /content/data/models\n",
        "!tar -xf models.tar.xz\n",
        "!mv /content/models/* /content/data/models/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yS6kXFIVXO9J",
        "outputId": "4ee2dcc4-e281-4c9e-c5c2-0901038c2485"
      },
      "execution_count": 5,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-09-20 14:17:52--  https://zenodo.org/record/7030145/files/models.tar.xz?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.184.117.155\n",
            "Connecting to zenodo.org (zenodo.org)|188.184.117.155|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1911937724 (1.8G) [application/octet-stream]\n",
            "Saving to: ‘models.tar.xz’\n",
            "\n",
            "models.tar.xz       100%[===================>]   1.78G  15.4MB/s    in 2m 33s  \n",
            "\n",
            "2022-09-20 14:20:28 (11.9 MB/s) - ‘models.tar.xz’ saved [1911937724/1911937724]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==2.10.0 subword-nmt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujyExeoEZ6VM",
        "outputId": "2d774aa0-6126-4ec3-eccc-cdfad1fef707"
      },
      "execution_count": 6,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==2.10.0\n",
            "  Downloading transformers-2.10.0-py3-none-any.whl (660 kB)\n",
            "\u001b[K     |████████████████████████████████| 660 kB 31.6 MB/s \n",
            "\u001b[?25hCollecting subword-nmt\n",
            "  Downloading subword_nmt-0.3.8-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.10.0) (3.8.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 55.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.10.0) (4.64.1)\n",
            "Collecting tokenizers==0.7.0\n",
            "  Downloading tokenizers-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 48.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.10.0) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.10.0) (2022.6.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.10.0) (1.21.6)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 43.8 MB/s \n",
            "\u001b[?25hCollecting mock\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.10.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.10.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.10.0) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.10.0) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.10.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.10.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.10.0) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=470f10a964bab34499a12436ce946ff31d354af82e1ac1969166c152e732c7ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, mock, transformers, subword-nmt\n",
            "Successfully installed mock-4.0.3 sacremoses-0.0.53 sentencepiece-0.1.97 subword-nmt-0.3.8 tokenizers-0.7.0 transformers-2.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "OiM2X0qPWds0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run gpt_conut_trainer.py file"
      ],
      "metadata": {
        "id": "9NPPvZ58sqY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import codecs\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import OpenAIGPTLMHeadModel\n",
        "\n",
        "GPT_CONUT_TRAINER_DIR = os.path.abspath('/content')#os.path.abspath(__file__)[: os.path.abspath(__file__).rindex('/') + 1]"
      ],
      "metadata": {
        "id": "YNQLj3YJWdTc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from src.models.gpt_conut import GPTCoNuTModel\n",
        "from src.dataloader.dictionary import Dictionary\n",
        "from src.dataloader.gpt_conut_data_loader import GPTCoNuTDataLoader"
      ],
      "metadata": {
        "id": "Ff_GgGKJWWyk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "DyuHj06gvReD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f'CUDA GPU availible : {torch.cuda.is_available()}')\n",
        "# DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "VpeQL1nBWybZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class GPTCoNuTTrainer():\n",
        "#     def __init__(self, train_loader, valid_loader, dictionary, gpt_file):\n",
        "#         gpt_loaded = torch.load(gpt_file)\n",
        "#         config = gpt_loaded['config']\n",
        "#         gpt_model = OpenAIGPTLMHeadModel(config).cuda()\n",
        "#         gpt_model.load_state_dict(gpt_loaded['model'])\n",
        "\n",
        "#         self.train_loader = train_loader\n",
        "#         self.valid_loader = valid_loader\n",
        "#         self.dictionary = dictionary\n",
        "\n",
        "#         self.batch_size = 12\n",
        "#         self.load_size = 1200   # load 1200 samples from training data every time\n",
        "\n",
        "#         self.gpt_model = gpt_model\n",
        "#         self.model = None\n",
        "#         self.hyper_parameter = {}\n",
        "#         self.optimizer = None\n",
        "#         self.current_train_step = 0\n",
        "#         self.val_loss = {}\n",
        "\n",
        "#     def shuffle_dataset(self):\n",
        "#         indices = [i for i in range(len(self.train_loader.dataset))]\n",
        "#         random.shuffle(indices)\n",
        "#         return indices\n",
        "\n",
        "#     def train_step(self, samples):\n",
        "#         self.model.train()\n",
        "#         self.current_train_step += 1\n",
        "#         self.optimizer.zero_grad()\n",
        "\n",
        "#         batch = self.train_loader.dataset.collater(samples)\n",
        "#         if torch.cuda.is_available():\n",
        "#             outputs = self.model(\n",
        "#                 batch['net_input']['src_tokens'].cuda(),\n",
        "#                 batch['net_input']['src_with_prev_context'].cuda(),\n",
        "#                 batch['net_input']['ctx_tokens'].cuda(),\n",
        "#                 prev_tokens_index=batch['target_index'].cuda(),\n",
        "#                 prev_tokens_with_context=batch['target_with_prev_context'].cuda(),\n",
        "#                 labels=batch['target'].cuda(),\n",
        "#             )\n",
        "#         else:\n",
        "#             outputs = self.model(\n",
        "#                 batch['net_input']['src_tokens'],\n",
        "#                 batch['net_input']['src_with_prev_context'],\n",
        "#                 batch['net_input']['ctx_tokens'],\n",
        "#                 prev_tokens_index=batch['target_index'],\n",
        "#                 prev_tokens_with_context=batch['target_with_prev_context'],\n",
        "#                 labels=batch['target'],\n",
        "#             )\n",
        "#         logits, avg_attn_scores, apr_loss, lm_loss = outputs[:4]\n",
        "#         loss = apr_loss + 0.3 * lm_loss\n",
        "#         loss.mean().backward()\n",
        "#         nn.utils.clip_grad_norm_(self.model.parameters(), 0.5, norm_type=2)\n",
        "#         self.optimizer.step()\n",
        "#         return loss.mean().item(), apr_loss.mean().item(), lm_loss.mean().item()\n",
        "\n",
        "#     def valid_step(self, samples):\n",
        "#         self.model.eval()\n",
        "#         batch = self.valid_loader.dataset.collater(samples)\n",
        "#         outputs = self.model(\n",
        "#             batch['net_input']['src_tokens'].cuda(),\n",
        "#             batch['net_input']['src_with_prev_context'].cuda(),\n",
        "#             batch['net_input']['ctx_tokens'].cuda(),\n",
        "#             prev_tokens_index=batch['target_index'].cuda(),\n",
        "#             prev_tokens_with_context=batch['target_with_prev_context'].cuda(),\n",
        "#             labels=batch['target'].cuda(),\n",
        "#         )\n",
        "#         logits, avg_attn_scores, apr_loss, lm_loss = outputs[:4]\n",
        "#         loss = apr_loss + 0.3 * lm_loss\n",
        "#         return loss.mean().item(), apr_loss.mean().item(), lm_loss.mean().item(), logits\n",
        "\n",
        "#     def validate_and_save(self, model_id, save_dir):\n",
        "#         oom = 0\n",
        "#         with torch.no_grad():\n",
        "#             val_loss, val_fconv_loss, val_lm_loss = [], [], []\n",
        "#             for i in range(0, self.valid_loader.total_size, self.batch_size):\n",
        "#                 samples = [self.valid_loader.dataset[j]\n",
        "#                            for j in range(i, min(len(self.valid_loader.dataset), i + self.batch_size))]\n",
        "#                 try:\n",
        "#                     loss, fconv_loss, lm_loss, logits = self.valid_step(samples)\n",
        "#                     val_loss.append(float(loss))\n",
        "#                     val_fconv_loss.append(float(fconv_loss))\n",
        "#                     val_lm_loss.append(float(lm_loss))\n",
        "#                 except Exception as e:\n",
        "#                     oom += 1\n",
        "\n",
        "#             info = 'val loss:{}, val apr_loss:{}, val lm_loss:{}, val ppl:{}, oom:{}'.format(\n",
        "#                 round(float(np.mean(val_loss)), 6),\n",
        "#                 round(float(np.mean(val_fconv_loss)), 6),\n",
        "#                 round(float(np.mean(val_lm_loss)), 6),\n",
        "#                 round(float(np.exp(np.mean(val_loss))), 6),\n",
        "#                 oom\n",
        "#             )\n",
        "#             print(info)\n",
        "\n",
        "#             val_loss = np.mean(val_fconv_loss)\n",
        "#             checkpoint = {\n",
        "#                 'model': self.model.state_dict(),\n",
        "#                 'optimizer': self.optimizer.state_dict(),\n",
        "#                 'current_step': self.current_train_step,\n",
        "#                 'config': self.model.config(),\n",
        "#                 'val_loss': val_loss,\n",
        "#             }\n",
        "#             torch.save(checkpoint, save_dir + 'gpt_conut_' + str(model_id) + '.pt')\n",
        "#             self.val_loss[model_id] = {\n",
        "#                 'val_loss': val_loss,\n",
        "#                 'hyper-parameter': str(self.hyper_parameter),\n",
        "#             }\n",
        "\n",
        "#         return val_loss\n",
        "\n",
        "#     def train(self, model_id, epochs, hyper_parameter, save_dir):\n",
        "#         self.hyper_parameter = hyper_parameter\n",
        "#         self.model = GPTCoNuTModel(\n",
        "#             self.dictionary, embed_dim=384, max_positions=1024,\n",
        "#             src_encoder_convolutions=self.hyper_parameter['src_encoder_convolutions'],\n",
        "#             ctx_encoder_convolutions=self.hyper_parameter['ctx_encoder_convolutions'],\n",
        "#             decoder_convolutions=self.hyper_parameter['decoder_convolutions'],\n",
        "#             dropout=self.hyper_parameter['dropout'], embed_model=self.gpt_model,\n",
        "#         ).cuda()\n",
        "#         self.optimizer = torch.optim.Adam(self.model.parameters(), lr=6.25e-5)\n",
        "#         # self.model = nn.DataParallel(self.model, device_ids=device_ids)\n",
        "        \n",
        "#         self.valid_loader.load_data(0, self.valid_loader.total_size)\n",
        "#         for epoch in range(epochs):\n",
        "#             start_time = time.time()\n",
        "#             for i in range(0, self.train_loader.total_size, self.load_size):\n",
        "#                 oom = 0\n",
        "#                 self.train_loader.load_data(i, i + self.load_size)\n",
        "#                 indices = self.shuffle_dataset()\n",
        "#                 train_loss, train_apr_loss, train_lm_loss = [], [], []\n",
        "\n",
        "#                 start, end = 0, 0\n",
        "#                 samples = []\n",
        "#                 max_src, max_ctx, max_tgt = 0, 0, 0\n",
        "#                 while end < len(self.train_loader.dataset):\n",
        "#                     sample = self.train_loader.dataset[indices[end]]\n",
        "#                     if max_ctx + len(sample['target']) >= 1023 \\\n",
        "#                             or max_tgt + len(sample['prev_context']) >= 1023 \\\n",
        "#                             or max_ctx + len(sample['source']) >= 1023 \\\n",
        "#                             or max_src + len(sample['prev_context']) >= 1023 \\\n",
        "#                             or end - start == self.batch_size:\n",
        "#                         try:\n",
        "#                             loss, apr_loss, lm_loss = self.train_step(samples)\n",
        "#                             train_loss.append(loss)\n",
        "#                             train_apr_loss.append(apr_loss)\n",
        "#                             train_lm_loss.append(lm_loss)\n",
        "#                         except Exception as e:\n",
        "#                             oom += 1\n",
        "\n",
        "#                         start = end\n",
        "#                         max_src, max_ctx, max_tgt = 0, 0, 0\n",
        "#                         samples = []\n",
        "#                         continue\n",
        "#                     max_src = max(max_src, len(sample['source']))\n",
        "#                     max_ctx = max(max_ctx, len(sample['prev_context']))\n",
        "#                     max_tgt = max(max_tgt, len(sample['target']))\n",
        "#                     end += 1\n",
        "#                     samples.append(sample)\n",
        "#                 if len(samples) > 0:\n",
        "#                     try:\n",
        "#                         loss, apr_loss, lm_loss = self.train_step(samples)\n",
        "#                         train_loss.append(loss)\n",
        "#                         train_apr_loss.append(apr_loss)\n",
        "#                         train_lm_loss.append(lm_loss)\n",
        "#                     except Exception as e:\n",
        "#                         oom += 1\n",
        "\n",
        "#                 if (i // self.load_size) % 10 == 0:\n",
        "#                     info = 'epoch:{}, load data:{}, lr:{}, loss:{}, apr_loss:{}, lm_loss:{}, time:{}s, oom:{}'.\\\n",
        "#                         format(epoch + 1, i + self.load_size,\n",
        "#                                round(self.optimizer.param_groups[0]['lr'], 10),\n",
        "#                                round(float(np.mean(train_loss)), 6),\n",
        "#                                round(float(np.mean(train_apr_loss)), 6),\n",
        "#                                round(float(np.mean(train_lm_loss)), 6),\n",
        "#                                int(time.time() - start_time), oom\n",
        "#                                )\n",
        "#                     start_time = time.time()\n",
        "#                     print(str(model_id) + ' ' + info)\n",
        "\n",
        "#                 if (i // self.load_size) % 100 == 0:\n",
        "#                     self.validate_and_save(model_id, save_dir)\n",
        "#         self.validate_and_save(model_id, save_dir)"
      ],
      "metadata": {
        "id": "KffOeTnGW1bG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if __name__ == '__main__':\n",
        "#     device_ids = [0, 1, 2, 3]\n",
        "#     os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
        "    \n",
        "#     vocab_file = GPT_CONUT_TRAINER_DIR + '/data/vocabulary/vocabulary.txt'\n",
        "#     train_file = GPT_CONUT_TRAINER_DIR + '/data/data/training_bpe.txt'\n",
        "#     valid_file = GPT_CONUT_TRAINER_DIR + '/data/data/validation_bpe.txt'\n",
        "#     gpt_file = GPT_CONUT_TRAINER_DIR + '/data/models/code_gpt.pt'\n",
        "\n",
        "#     dictionary = Dictionary(vocab_file, min_cnt=0)\n",
        "#     print('dictionary initialized, vocab size:{}'.format(len(dictionary)))\n",
        "\n",
        "#     train_loader = GPTCoNuTDataLoader(train_file, dictionary)\n",
        "#     valid_loader = GPTCoNuTDataLoader(valid_file, dictionary)\n",
        "#     print('data loader initialized, train size:{}, validate size:{}'.\n",
        "#           format(train_loader.total_size, valid_loader.total_size))\n",
        "\n",
        "#     trainer = GPTCoNuTTrainer(train_loader, valid_loader, dictionary, gpt_file)\n",
        "\n",
        "#     hyper_parameter = {\n",
        "#         'src_encoder_convolutions': ((192, 5),) * 1,\n",
        "#         'ctx_encoder_convolutions': ((384, 5),) * 1,\n",
        "#         'decoder_convolutions': ((192, 5),) * 1,\n",
        "#         'dropout': 0.1,\n",
        "#     }\n",
        "#     model_id = 1\n",
        "#     epochs = 5\n",
        "#     trainer.train(model_id, epochs, hyper_parameter, save_dir=GPT_CONUT_TRAINER_DIR + '/data/models/')"
      ],
      "metadata": {
        "id": "7doawc2lW8o5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run gpt_fconv_trainer.py file"
      ],
      "metadata": {
        "id": "hDGLIYMss1kL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import codecs\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import OpenAIGPTLMHeadModel\n",
        "\n",
        "# GPT_FCONV_TRAINER_DIR = os.path.abspath(__file__)[: os.path.abspath(__file__).rindex('/') + 1]\n",
        "GPT_FCONV_TRAINER_DIR = os.path.abspath('/content')\n",
        "\n",
        "from src.models.gpt_fconv import GPTFConvModel\n",
        "from src.dataloader.dictionary import Dictionary\n",
        "from src.dataloader.gpt_fconv_data_loader import GPTFConvDataLoader\n",
        "\n",
        "\n",
        "# class GPTFConvTrainer():\n",
        "#     def __init__(self, train_loader, valid_loader, dictionary, gpt_file):\n",
        "#         gpt_loaded = torch.load(gpt_file)\n",
        "#         config = gpt_loaded['config']\n",
        "#         gpt_model = OpenAIGPTLMHeadModel(config).cuda()\n",
        "#         gpt_model.load_state_dict(gpt_loaded['model'])\n",
        "\n",
        "#         self.train_loader = train_loader\n",
        "#         self.valid_loader = valid_loader\n",
        "#         self.dictionary = dictionary\n",
        "\n",
        "#         self.batch_size = 12\n",
        "#         self.load_size = 1200\n",
        "\n",
        "#         self.gpt_model = gpt_model\n",
        "#         self.model = None\n",
        "#         self.hyper_parameter = {}\n",
        "#         self.hyper_parameter_set = {'{}'}\n",
        "#         self.optimizer = None\n",
        "#         self.current_train_step = 0\n",
        "#         self.val_loss = {}\n",
        "\n",
        "#     def shuffle_dataset(self):\n",
        "#         indices = [i for i in range(len(self.train_loader.dataset))]\n",
        "#         random.shuffle(indices)\n",
        "#         return indices\n",
        "\n",
        "#     def train_step(self, samples):\n",
        "#         self.model.train()\n",
        "#         self.current_train_step += 1\n",
        "#         self.optimizer.zero_grad()\n",
        "\n",
        "#         batch = self.train_loader.dataset.collater(samples)\n",
        "#         if torch.cuda.is_available():\n",
        "#             outputs = self.model(\n",
        "#                 batch['net_input']['src_tokens'].cuda(),\n",
        "#                 batch['net_input']['src_with_prev_context'].cuda(),\n",
        "#                 prev_tokens_index=batch['target_index'].cuda(),\n",
        "#                 prev_tokens_with_context=batch['target_with_prev_context'].cuda(),\n",
        "#                 labels=batch['target'].cuda(),\n",
        "#             )\n",
        "\n",
        "#         logits, avg_attn_scores, apr_loss, lm_loss = outputs[:4]\n",
        "#         loss = apr_loss + 0.3 * lm_loss\n",
        "#         loss.mean().backward()\n",
        "#         nn.utils.clip_grad_norm_(self.model.parameters(), 0.5, norm_type=2)\n",
        "#         self.optimizer.step()\n",
        "#         return loss.mean().item(), apr_loss.mean().item(), lm_loss.mean().item()\n",
        "\n",
        "#     def valid_step(self, samples):\n",
        "#         self.model.eval()\n",
        "#         batch = self.valid_loader.dataset.collater(samples)\n",
        "#         outputs = self.model(\n",
        "#             batch['net_input']['src_tokens'].cuda(),\n",
        "#             batch['net_input']['src_with_prev_context'].cuda(),\n",
        "#             prev_tokens_index=batch['target_index'].cuda(),\n",
        "#             prev_tokens_with_context=batch['target_with_prev_context'].cuda(),\n",
        "#             labels=batch['target'].cuda(),\n",
        "#         )\n",
        "#         logits, avg_attn_scores, apr_loss, lm_loss = outputs[:4]\n",
        "#         loss = apr_loss + 0.3 * lm_loss\n",
        "#         return loss.mean().item(), apr_loss.mean().item(), lm_loss.mean().item(), logits\n",
        "\n",
        "#     def validate_and_save(self, model_id, save_dir):\n",
        "#         oom = 0\n",
        "#         with torch.no_grad():\n",
        "#             val_loss, val_fconv_loss, val_lm_loss = [], [], []\n",
        "#             for i in range(0, self.valid_loader.total_size, self.batch_size):\n",
        "#                 samples = [self.valid_loader.dataset[j]\n",
        "#                            for j in range(i, min(len(self.valid_loader.dataset), i + self.batch_size))]\n",
        "#                 try:\n",
        "#                     loss, fconv_loss, lm_loss, logits = self.valid_step(samples)\n",
        "#                     val_loss.append(float(loss))\n",
        "#                     val_fconv_loss.append(float(fconv_loss))\n",
        "#                     val_lm_loss.append(float(lm_loss))\n",
        "#                 except Exception as e:\n",
        "#                     oom += 1\n",
        "\n",
        "#             info = 'val loss:{}, val apr_loss:{}, val lm_loss:{}, val ppl:{}, oom:{}'.format(\n",
        "#                 round(float(np.mean(val_loss)), 6),\n",
        "#                 round(float(np.mean(val_fconv_loss)), 6),\n",
        "#                 round(float(np.mean(val_lm_loss)), 6),\n",
        "#                 round(float(np.exp(np.mean(val_loss))), 6),\n",
        "#                 oom\n",
        "#             )\n",
        "#             print(info)\n",
        "\n",
        "#             val_loss = np.mean(val_fconv_loss)\n",
        "#             checkpoint = {\n",
        "#                 'model': self.model.state_dict(),\n",
        "#                 'optimizer': self.optimizer.state_dict(),\n",
        "#                 'current_step': self.current_train_step,\n",
        "#                 'config': self.model.config(),\n",
        "#                 'val_loss': val_loss,\n",
        "#             }\n",
        "#             torch.save(checkpoint, save_dir + 'gpt_fconv_' + str(model_id) + '.pt')\n",
        "#             self.val_loss[model_id] = {\n",
        "#                 'val_loss': val_loss,\n",
        "#                 'hyper-parameter': str(self.hyper_parameter),\n",
        "#             }\n",
        "#         return val_loss\n",
        "\n",
        "#     def train(self, model_id, epochs, hyper_parameter, save_dir):\n",
        "#         self.hyper_parameter = hyper_parameter\n",
        "#         self.model = GPTFConvModel(\n",
        "#                 self.dictionary, embed_dim=384, max_positions=1024,\n",
        "#                 encoder_convolutions=self.hyper_parameter['encoder_convolutions'],\n",
        "#                 decoder_convolutions=self.hyper_parameter['decoder_convolutions'],\n",
        "#                 dropout=self.hyper_parameter['dropout'], embed_model=self.gpt_model,\n",
        "#             ).cuda()\n",
        "#         self.optimizer = torch.optim.Adam(self.model.parameters(), lr=6.25e-5)\n",
        "#         # self.model = nn.DataParallel(self.model, device_ids=device_ids)\n",
        "        \n",
        "#         self.valid_loader.load_data(0, self.valid_loader.total_size)\n",
        "#         for epoch in range(epochs):\n",
        "#             start_time = time.time()\n",
        "#             for i in range(0, self.train_loader.total_size, self.load_size):\n",
        "#                 oom = 0\n",
        "#                 self.train_loader.load_data(i, i + self.load_size)\n",
        "#                 indices = self.shuffle_dataset()\n",
        "#                 train_loss, train_apr_loss, train_lm_loss = [], [], []\n",
        "\n",
        "#                 start, end = 0, 0\n",
        "#                 samples = []\n",
        "#                 max_src, max_ctx, max_tgt = 0, 0, 0\n",
        "#                 while end < len(self.train_loader.dataset):\n",
        "#                     sample = self.train_loader.dataset[indices[end]]\n",
        "#                     if max_ctx + len(sample['target']) >= 1023 \\\n",
        "#                             or max_tgt + len(sample['prev_context']) >= 1023 \\\n",
        "#                             or max_ctx + len(sample['source']) >= 1023 \\\n",
        "#                             or max_src + len(sample['prev_context']) >= 1023 \\\n",
        "#                             or end - start == self.batch_size:\n",
        "#                         try:\n",
        "#                             loss, apr_loss, lm_loss = self.train_step(samples)\n",
        "#                             train_loss.append(loss)\n",
        "#                             train_apr_loss.append(apr_loss)\n",
        "#                             train_lm_loss.append(lm_loss)\n",
        "#                         except Exception as e:\n",
        "#                             oom += 1\n",
        "\n",
        "#                         start = end\n",
        "#                         max_src, max_ctx, max_tgt = 0, 0, 0\n",
        "#                         samples = []\n",
        "#                         continue\n",
        "#                     max_src = max(max_src, len(sample['source']))\n",
        "#                     max_ctx = max(max_ctx, len(sample['prev_context']))\n",
        "#                     max_tgt = max(max_tgt, len(sample['target']))\n",
        "#                     end += 1\n",
        "#                     samples.append(sample)\n",
        "#                 if len(samples) > 0:\n",
        "#                     try:\n",
        "#                         loss, apr_loss, lm_loss = self.train_step(samples)\n",
        "#                         train_loss.append(loss)\n",
        "#                         train_apr_loss.append(apr_loss)\n",
        "#                         train_lm_loss.append(lm_loss)\n",
        "#                     except Exception as e:\n",
        "#                         oom += 1\n",
        "\n",
        "#                 if (i // self.load_size) % 10 == 0:\n",
        "#                     info = 'epoch:{}, load data:{}, lr:{}, loss:{}, apr_loss:{}, lm_loss:{}, time:{}s, oom:{}'.\\\n",
        "#                         format(epoch + 1, i + self.load_size,\n",
        "#                                round(self.optimizer.param_groups[0]['lr'], 10),\n",
        "#                                round(float(np.mean(train_loss)), 6),\n",
        "#                                round(float(np.mean(train_apr_loss)), 6),\n",
        "#                                round(float(np.mean(train_lm_loss)), 6),\n",
        "#                                int(time.time() - start_time), oom\n",
        "#                                )\n",
        "#                     start_time = time.time()\n",
        "#                     print(str(model_id) + ' ' + info)\n",
        "\n",
        "#                 if (i // self.load_size) % 100 == 0:\n",
        "#                     self.validate_and_save(model_id, save_dir)\n",
        "#         self.validate_and_save(model_id, save_dir)\n",
        "\n"
      ],
      "metadata": {
        "id": "JDCQxfDh4pXX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# device_ids = [0, 1, 2, 3]\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
        "    \n",
        "# vocab_file = GPT_FCONV_TRAINER_DIR + '/data/vocabulary/vocabulary.txt'\n",
        "# train_file = GPT_FCONV_TRAINER_DIR + '/data/data/training_bpe.txt'\n",
        "# valid_file = GPT_FCONV_TRAINER_DIR + '/data/data/validation_bpe.txt'\n",
        "# gpt_file = GPT_FCONV_TRAINER_DIR + '/data/models/code_gpt.pt'\n",
        "\n",
        "# dictionary = Dictionary(vocab_file, min_cnt=0)\n",
        "# print('dictionary initialized, vocab size:{}'.format(len(dictionary)))\n",
        "\n",
        "# train_loader = GPTFConvDataLoader(train_file, dictionary)\n",
        "# valid_loader = GPTFConvDataLoader(valid_file, dictionary)\n",
        "# print('data loader initialized, train size:{}, validate size:{}'.\n",
        "#       format(train_loader.total_size, valid_loader.total_size))\n",
        "\n",
        "# trainer = GPTFConvTrainer(train_loader, valid_loader, dictionary, gpt_file)\n",
        "\n",
        "# hyper_parameter = {\n",
        "#     'encoder_convolutions': ((192, 5),) * 1,\n",
        "#     'decoder_convolutions': ((192, 5),) * 1,\n",
        "#     'dropout': 0.1,\n",
        "# }\n",
        "# trainer.train(1, 2, hyper_parameter, save_dir=GPT_FCONV_TRAINER_DIR + '/data/models/')\n"
      ],
      "metadata": {
        "id": "caPCS1GvuDXM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run generator.py file"
      ],
      "metadata": {
        "id": "mvFxGHgos-t9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import codecs\n",
        "import torch\n",
        "import sys\n",
        "import os\n",
        "from transformers import OpenAIGPTLMHeadModel\n",
        "\n",
        "# GENERATOR_DIR = os.path.abspath(__file__)[: os.path.abspath(__file__).rindex('/') + 1]\n",
        "GENERATOR_DIR = os.path.abspath('/content')\n",
        "# sys.path.append(GENERATOR_DIR + '/models/')\n",
        "# sys.path.append(GENERATOR_DIR + '/dataloader/')\n",
        "# sys.path.append(GENERATOR_DIR + '/tester/')\n",
        "from src.dataloader.gpt_conut_data_loader import GPTCoNuTDataLoader\n",
        "from src.dataloader.gpt_fconv_data_loader import GPTFConvDataLoader\n",
        "from src.dataloader.identifier_data_loader import IdentifierDataLoader\n",
        "from src.dataloader.dictionary import Dictionary\n",
        "from src.models.gpt_conut import GPTCoNuTModel\n",
        "from src.models.gpt_fconv import GPTFConvModel\n",
        "from src.tester.beamsearch import BeamSearch\n",
        "\n",
        "\n",
        "class Generator():\n",
        "    def __init__(self, model, model_type, dictionary, data_loader, beam_size=10):\n",
        "        self.model = model\n",
        "        self.dictionary = dictionary\n",
        "        self.data_loader = data_loader\n",
        "        self.beam_size = beam_size\n",
        "        self.beamsearch = BeamSearch(model, model_type, dictionary, beam_size)\n",
        "        print(self.model, beam_size)\n",
        "\n",
        "    def generate(self, output_path):\n",
        "        wp = codecs.open(output_path, 'w', 'utf-8')\n",
        "        self.data_loader.load_data(0, self.data_loader.total_size)\n",
        "        for i in range(self.data_loader.total_size):\n",
        "            print(i, '/', self.data_loader.total_size)\n",
        "            data = self.data_loader.dataset[i]\n",
        "            if True:\n",
        "                self.beamsearch.beam_size = self.beam_size\n",
        "                sample = self.data_loader.dataset.collater([data])\n",
        "                with torch.no_grad():\n",
        "                    if isinstance(self.model, GPTCoNuTModel):\n",
        "                        hypothesis = self.beamsearch.generate_gpt_conut(sample)\n",
        "                    elif isinstance(self.model, GPTFConvModel):\n",
        "                        hypothesis = self.beamsearch.generate_gpt_fconv(sample)\n",
        "            # except Exception as e:\n",
        "            #    print(e)\n",
        "            #    continue\n",
        "            id = str(sample['id'].item())\n",
        "            wp.write('S-{}\\t'.format(id))\n",
        "            wp.write(self.dictionary.string(data['source']) + '\\n')\n",
        "            wp.write('T-{}\\t'.format(id))\n",
        "            wp.write(self.dictionary.string(data['target']) + '\\n')\n",
        "            for h in hypothesis:\n",
        "                wp.write('H-{}\\t{}\\t'.format(id, str(h['final_score'])))\n",
        "                wp.write(self.dictionary.string(h['hypo']) + '\\n')\n",
        "                wp.write('P-{}\\t'.format(id))\n",
        "                wp.write(' '.join(str(round(s.item(), 4)) for s in h['score']) + '\\n')\n",
        "        wp.close()\n",
        "\n",
        "\n",
        "def generate_gpt_conut(vocab_file, model_file, input_file, identifier_txt_file, identifier_token_file, output_file, beam_size):\n",
        "    dictionary = Dictionary(vocab_file, min_cnt=0)\n",
        "    print(len(dictionary))\n",
        "    loaded = torch.load(model_file, map_location='cpu')\n",
        "    config = loaded['config']\n",
        "    gpt_config = config['embed_model_config']\n",
        "    gpt_config.attn_pdrop = 0\n",
        "    gpt_config.embd_pdrop = 0\n",
        "    gpt_config.resid_pdrop = 0\n",
        "    gpt_model = OpenAIGPTLMHeadModel(gpt_config)\n",
        "    model = GPTCoNuTModel(\n",
        "        dictionary=dictionary, embed_dim=config['embed_dim'],\n",
        "        max_positions=config['max_positions'],\n",
        "        src_encoder_convolutions=config['src_encoder_convolutions'],\n",
        "        ctx_encoder_convolutions=config['ctx_encoder_convolutions'],\n",
        "        decoder_convolutions=config['decoder_convolutions'],\n",
        "        dropout=0, embed_model=gpt_model,\n",
        "    )\n",
        "\n",
        "    model.load_state_dict(loaded['model'])\n",
        "    identifier_loader = IdentifierDataLoader(\n",
        "        dictionary, identifier_token_file, identifier_txt_file\n",
        "    )\n",
        "    data_loader = GPTCoNuTDataLoader(\n",
        "        input_file, dictionary,\n",
        "        identifier_loader=identifier_loader\n",
        "    )\n",
        "    generator = Generator(model, \"conut\", dictionary, data_loader, beam_size=beam_size)\n",
        "    print('start generate')\n",
        "    generator.generate(output_file)\n",
        "\n",
        "\n",
        "def generate_gpt_fconv(vocab_file, model_file, input_file, identifier_txt_file, identifier_token_file, output_file, beam_size):\n",
        "    dictionary = Dictionary(vocab_file, min_cnt=0)\n",
        "    print(len(dictionary))\n",
        "    loaded = torch.load(\n",
        "        model_file, map_location='cpu'\n",
        "    )\n",
        "    config = loaded['config']\n",
        "    gpt_config = config['embed_model_config']\n",
        "    gpt_config.attn_pdrop = 0\n",
        "    gpt_config.embd_pdrop = 0\n",
        "    gpt_config.resid_pdrop = 0\n",
        "    gpt_model = OpenAIGPTLMHeadModel(gpt_config)\n",
        "    model = GPTFConvModel(\n",
        "        dictionary=dictionary, embed_dim=config['embed_dim'],\n",
        "        max_positions=config['max_positions'],\n",
        "        encoder_convolutions=config['encoder_convolutions'],\n",
        "        decoder_convolutions=config['decoder_convolutions'],\n",
        "        dropout=0, embed_model=gpt_model,\n",
        "    )\n",
        "    model.load_state_dict(loaded['model'])\n",
        "    identifier_loader = IdentifierDataLoader(\n",
        "        dictionary, identifier_token_file, identifier_txt_file\n",
        "    )\n",
        "    data_loader = GPTFConvDataLoader(\n",
        "        input_file, dictionary,\n",
        "        identifier_loader=identifier_loader\n",
        "    )\n",
        "    generator = Generator(model, \"fconv\", dictionary, data_loader, beam_size=beam_size)\n",
        "    print('start generate')\n",
        "    generator.generate(output_file)\n"
      ],
      "metadata": {
        "id": "ZaRpLCXstEU6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_file = GENERATOR_DIR + '/data/vocabulary/vocabulary.txt'\n",
        "input_file = GENERATOR_DIR + '/candidate_patches/QuixBugs/quixbugs_bpe.txt'\n",
        "identifier_txt_file = GENERATOR_DIR + '/candidate_patches/QuixBugs/identifier.txt'\n",
        "identifier_token_file = GENERATOR_DIR + '/candidate_patches/QuixBugs/identifier.tokens'\n",
        "beam_size = 1000\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
        "\n",
        "model_file = GENERATOR_DIR + '/data/models/gpt_fconv_1.pt'\n",
        "output_file = GENERATOR_DIR + '/data/patches/gpt_fconv_1.txt'\n",
        "generate_gpt_fconv(vocab_file, model_file, input_file, identifier_txt_file, identifier_token_file, output_file, beam_size)\n",
        "\n",
        "model_file = GENERATOR_DIR + '/data/models/gpt_conut_1.pt'\n",
        "output_file = GENERATOR_DIR + '/data/patches/gpt_conut_1.txt'\n",
        "generate_gpt_conut(vocab_file, model_file, input_file, identifier_txt_file, identifier_token_file, output_file, beam_size)\n",
        "\n"
      ],
      "metadata": {
        "id": "hQI1Te-4uQ56",
        "outputId": "33f18fad-efe3-44b9-e202-366818298c02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50061\n",
            "GPTFConvModel(\n",
            "  (embed_model): OpenAIGPTLMHeadModel(\n",
            "    (transformer): OpenAIGPTModel(\n",
            "      (tokens_embed): Embedding(50061, 384)\n",
            "      (positions_embed): Embedding(1024, 384)\n",
            "      (drop): Dropout(p=0, inplace=False)\n",
            "      (h): ModuleList(\n",
            "        (0): Block(\n",
            "          (attn): Attention(\n",
            "            (c_attn): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (attn_dropout): Dropout(p=0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): MLP(\n",
            "            (c_fc): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (1): Block(\n",
            "          (attn): Attention(\n",
            "            (c_attn): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (attn_dropout): Dropout(p=0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): MLP(\n",
            "            (c_fc): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (2): Block(\n",
            "          (attn): Attention(\n",
            "            (c_attn): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (attn_dropout): Dropout(p=0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): MLP(\n",
            "            (c_fc): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (3): Block(\n",
            "          (attn): Attention(\n",
            "            (c_attn): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (attn_dropout): Dropout(p=0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): MLP(\n",
            "            (c_fc): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (4): Block(\n",
            "          (attn): Attention(\n",
            "            (c_attn): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (attn_dropout): Dropout(p=0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): MLP(\n",
            "            (c_fc): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (5): Block(\n",
            "          (attn): Attention(\n",
            "            (c_attn): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (attn_dropout): Dropout(p=0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): MLP(\n",
            "            (c_fc): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (6): Block(\n",
            "          (attn): Attention(\n",
            "            (c_attn): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (attn_dropout): Dropout(p=0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): MLP(\n",
            "            (c_fc): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (7): Block(\n",
            "          (attn): Attention(\n",
            "            (c_attn): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (attn_dropout): Dropout(p=0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): MLP(\n",
            "            (c_fc): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (lm_head): Linear(in_features=384, out_features=50061, bias=False)\n",
            "  )\n",
            "  (encoder): GPTFConvEncoder(\n",
            "    (embed_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "    (fc1): Linear(in_features=384, out_features=352, bias=True)\n",
            "    (convolutions): ModuleList(\n",
            "      (0): ConvTBC()\n",
            "      (1): ConvTBC()\n",
            "      (2): ConvTBC()\n",
            "      (3): ConvTBC()\n",
            "      (4): ConvTBC()\n",
            "      (5): ConvTBC()\n",
            "    )\n",
            "    (attentions): ModuleList(\n",
            "      (0): None\n",
            "      (1): None\n",
            "      (2): None\n",
            "      (3): None\n",
            "      (4): None\n",
            "      (5): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=352, out_features=352, bias=True)\n",
            "        (out_projection): Linear(in_features=352, out_features=352, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (norms): ModuleList(\n",
            "      (0): LayerNorm((352,), eps=1e-05, elementwise_affine=True)\n",
            "      (1): LayerNorm((352,), eps=1e-05, elementwise_affine=True)\n",
            "      (2): LayerNorm((352,), eps=1e-05, elementwise_affine=True)\n",
            "      (3): LayerNorm((352,), eps=1e-05, elementwise_affine=True)\n",
            "      (4): LayerNorm((352,), eps=1e-05, elementwise_affine=True)\n",
            "      (5): LayerNorm((352,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (fc2): Linear(in_features=352, out_features=384, bias=True)\n",
            "  )\n",
            "  (decoder): GPTFConvDecoder(\n",
            "    (embed_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "    (fc1): Linear(in_features=384, out_features=416, bias=True)\n",
            "    (convolutions): ModuleList(\n",
            "      (0): ConvTBC()\n",
            "      (1): ConvTBC()\n",
            "      (2): ConvTBC()\n",
            "      (3): ConvTBC()\n",
            "      (4): ConvTBC()\n",
            "      (5): ConvTBC()\n",
            "    )\n",
            "    (attentions): ModuleList(\n",
            "      (0): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=416, out_features=384, bias=True)\n",
            "        (out_projection): Linear(in_features=384, out_features=416, bias=True)\n",
            "      )\n",
            "      (1): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=416, out_features=384, bias=True)\n",
            "        (out_projection): Linear(in_features=384, out_features=416, bias=True)\n",
            "      )\n",
            "      (2): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=416, out_features=384, bias=True)\n",
            "        (out_projection): Linear(in_features=384, out_features=416, bias=True)\n",
            "      )\n",
            "      (3): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=416, out_features=384, bias=True)\n",
            "        (out_projection): Linear(in_features=384, out_features=416, bias=True)\n",
            "      )\n",
            "      (4): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=416, out_features=384, bias=True)\n",
            "        (out_projection): Linear(in_features=384, out_features=416, bias=True)\n",
            "      )\n",
            "      (5): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=416, out_features=384, bias=True)\n",
            "        (out_projection): Linear(in_features=384, out_features=416, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (norms): ModuleList(\n",
            "      (0): LayerNorm((416,), eps=1e-05, elementwise_affine=True)\n",
            "      (1): LayerNorm((416,), eps=1e-05, elementwise_affine=True)\n",
            "      (2): LayerNorm((416,), eps=1e-05, elementwise_affine=True)\n",
            "      (3): LayerNorm((416,), eps=1e-05, elementwise_affine=True)\n",
            "      (4): LayerNorm((416,), eps=1e-05, elementwise_affine=True)\n",
            "      (5): LayerNorm((416,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (fcg): Linear(in_features=800, out_features=1, bias=True)\n",
            "    (fc2): Linear(in_features=416, out_features=50061, bias=True)\n",
            "  )\n",
            ") 1000\n",
            "start generate\n",
            "0 / 39\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 / 39\n",
            "2 / 39\n",
            "3 / 39\n",
            "4 / 39\n",
            "5 / 39\n",
            "6 / 39\n",
            "7 / 39\n",
            "8 / 39\n",
            "9 / 39\n",
            "10 / 39\n",
            "11 / 39\n",
            "12 / 39\n",
            "13 / 39\n",
            "14 / 39\n",
            "15 / 39\n",
            "16 / 39\n",
            "17 / 39\n",
            "18 / 39\n",
            "19 / 39\n",
            "20 / 39\n",
            "21 / 39\n",
            "22 / 39\n",
            "23 / 39\n",
            "24 / 39\n",
            "25 / 39\n",
            "26 / 39\n",
            "27 / 39\n",
            "28 / 39\n",
            "29 / 39\n",
            "30 / 39\n",
            "31 / 39\n",
            "32 / 39\n",
            "33 / 39\n",
            "34 / 39\n",
            "35 / 39\n",
            "36 / 39\n",
            "37 / 39\n",
            "38 / 39\n",
            "50061\n",
            "GPTCoNuTModel(\n",
            "  (embed_model): OpenAIGPTLMHeadModel(\n",
            "    (transformer): OpenAIGPTModel(\n",
            "      (tokens_embed): Embedding(50061, 384)\n",
            "      (positions_embed): Embedding(1024, 384)\n",
            "      (drop): Dropout(p=0, inplace=False)\n",
            "      (h): ModuleList(\n",
            "        (0): Block(\n",
            "          (attn): Attention(\n",
            "            (c_attn): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (attn_dropout): Dropout(p=0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): MLP(\n",
            "            (c_fc): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (1): Block(\n",
            "          (attn): Attention(\n",
            "            (c_attn): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (attn_dropout): Dropout(p=0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): MLP(\n",
            "            (c_fc): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (2): Block(\n",
            "          (attn): Attention(\n",
            "            (c_attn): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (attn_dropout): Dropout(p=0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): MLP(\n",
            "            (c_fc): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (3): Block(\n",
            "          (attn): Attention(\n",
            "            (c_attn): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (attn_dropout): Dropout(p=0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): MLP(\n",
            "            (c_fc): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (4): Block(\n",
            "          (attn): Attention(\n",
            "            (c_attn): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (attn_dropout): Dropout(p=0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): MLP(\n",
            "            (c_fc): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (5): Block(\n",
            "          (attn): Attention(\n",
            "            (c_attn): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (attn_dropout): Dropout(p=0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): MLP(\n",
            "            (c_fc): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (6): Block(\n",
            "          (attn): Attention(\n",
            "            (c_attn): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (attn_dropout): Dropout(p=0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): MLP(\n",
            "            (c_fc): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (7): Block(\n",
            "          (attn): Attention(\n",
            "            (c_attn): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (attn_dropout): Dropout(p=0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (ln_1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): MLP(\n",
            "            (c_fc): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (ln_2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (lm_head): Linear(in_features=384, out_features=50061, bias=False)\n",
            "  )\n",
            "  (encoder): GPTCoNuTEncoder(\n",
            "    (src_encoder): GPTFConvEncoder(\n",
            "      (embed_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc1): Linear(in_features=384, out_features=352, bias=True)\n",
            "      (convolutions): ModuleList(\n",
            "        (0): ConvTBC()\n",
            "        (1): ConvTBC()\n",
            "        (2): ConvTBC()\n",
            "        (3): ConvTBC()\n",
            "        (4): ConvTBC()\n",
            "      )\n",
            "      (attentions): ModuleList(\n",
            "        (0): None\n",
            "        (1): None\n",
            "        (2): None\n",
            "        (3): None\n",
            "        (4): AttentionLayer(\n",
            "          (in_projection): Linear(in_features=352, out_features=352, bias=True)\n",
            "          (out_projection): Linear(in_features=352, out_features=352, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norms): ModuleList(\n",
            "        (0): LayerNorm((352,), eps=1e-05, elementwise_affine=True)\n",
            "        (1): LayerNorm((352,), eps=1e-05, elementwise_affine=True)\n",
            "        (2): LayerNorm((352,), eps=1e-05, elementwise_affine=True)\n",
            "        (3): LayerNorm((352,), eps=1e-05, elementwise_affine=True)\n",
            "        (4): LayerNorm((352,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (fc2): Linear(in_features=352, out_features=384, bias=True)\n",
            "    )\n",
            "    (context_encoder): GPTFConvEncoder(\n",
            "      (embed_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "      (fc1): Linear(in_features=384, out_features=352, bias=True)\n",
            "      (convolutions): ModuleList(\n",
            "        (0): ConvTBC()\n",
            "        (1): ConvTBC()\n",
            "        (2): ConvTBC()\n",
            "        (3): ConvTBC()\n",
            "        (4): ConvTBC()\n",
            "      )\n",
            "      (attentions): ModuleList(\n",
            "        (0): None\n",
            "        (1): None\n",
            "        (2): None\n",
            "        (3): None\n",
            "        (4): AttentionLayer(\n",
            "          (in_projection): Linear(in_features=352, out_features=352, bias=True)\n",
            "          (out_projection): Linear(in_features=352, out_features=352, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (norms): ModuleList(\n",
            "        (0): LayerNorm((352,), eps=1e-05, elementwise_affine=True)\n",
            "        (1): LayerNorm((352,), eps=1e-05, elementwise_affine=True)\n",
            "        (2): LayerNorm((352,), eps=1e-05, elementwise_affine=True)\n",
            "        (3): LayerNorm((352,), eps=1e-05, elementwise_affine=True)\n",
            "        (4): LayerNorm((352,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (fc2): Linear(in_features=352, out_features=384, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (decoder): GPTFConvDecoder(\n",
            "    (embed_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "    (fc1): Linear(in_features=384, out_features=224, bias=True)\n",
            "    (convolutions): ModuleList(\n",
            "      (0): ConvTBC()\n",
            "      (1): ConvTBC()\n",
            "      (2): ConvTBC()\n",
            "      (3): ConvTBC()\n",
            "      (4): ConvTBC()\n",
            "    )\n",
            "    (attentions): ModuleList(\n",
            "      (0): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=224, out_features=384, bias=True)\n",
            "        (out_projection): Linear(in_features=384, out_features=224, bias=True)\n",
            "      )\n",
            "      (1): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=224, out_features=384, bias=True)\n",
            "        (out_projection): Linear(in_features=384, out_features=224, bias=True)\n",
            "      )\n",
            "      (2): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=224, out_features=384, bias=True)\n",
            "        (out_projection): Linear(in_features=384, out_features=224, bias=True)\n",
            "      )\n",
            "      (3): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=224, out_features=384, bias=True)\n",
            "        (out_projection): Linear(in_features=384, out_features=224, bias=True)\n",
            "      )\n",
            "      (4): AttentionLayer(\n",
            "        (in_projection): Linear(in_features=224, out_features=384, bias=True)\n",
            "        (out_projection): Linear(in_features=384, out_features=224, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (norms): ModuleList(\n",
            "      (0): LayerNorm((224,), eps=1e-05, elementwise_affine=True)\n",
            "      (1): LayerNorm((224,), eps=1e-05, elementwise_affine=True)\n",
            "      (2): LayerNorm((224,), eps=1e-05, elementwise_affine=True)\n",
            "      (3): LayerNorm((224,), eps=1e-05, elementwise_affine=True)\n",
            "      (4): LayerNorm((224,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (fcg): Linear(in_features=608, out_features=1, bias=True)\n",
            "    (fc2): Linear(in_features=224, out_features=50061, bias=True)\n",
            "  )\n",
            ") 1000\n",
            "start generate\n",
            "0 / 39\n",
            "1 / 39\n",
            "2 / 39\n",
            "3 / 39\n",
            "4 / 39\n",
            "5 / 39\n",
            "6 / 39\n",
            "7 / 39\n",
            "8 / 39\n",
            "9 / 39\n",
            "10 / 39\n",
            "11 / 39\n",
            "12 / 39\n",
            "13 / 39\n",
            "14 / 39\n",
            "15 / 39\n",
            "16 / 39\n",
            "17 / 39\n",
            "18 / 39\n",
            "19 / 39\n",
            "20 / 39\n",
            "21 / 39\n",
            "22 / 39\n",
            "23 / 39\n",
            "24 / 39\n",
            "25 / 39\n",
            "26 / 39\n",
            "27 / 39\n",
            "28 / 39\n",
            "29 / 39\n",
            "30 / 39\n",
            "31 / 39\n",
            "32 / 39\n",
            "33 / 39\n",
            "34 / 39\n",
            "35 / 39\n",
            "36 / 39\n",
            "37 / 39\n",
            "38 / 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import codecs\n",
        "import json\n",
        "import os\n",
        "\n",
        "RERANK_DIR = os.path.abspath('/content')\n",
        "\n",
        "\n",
        "def read_defects4j_meta(meta_path):\n",
        "    fp = codecs.open(meta_path, 'r', 'utf-8')\n",
        "    meta = []\n",
        "    for l in fp.readlines():\n",
        "        proj, bug_id, path, start_loc, end_loc = l.strip().split('\\t')\n",
        "        meta.append([\n",
        "            proj, bug_id, path, start_loc, end_loc + 1\n",
        "        ])\n",
        "    return meta\n",
        "\n",
        "\n",
        "def read_quixbugs_meta(meta_path):\n",
        "    fp = codecs.open(meta_path, 'r', 'utf-8')\n",
        "    meta = []\n",
        "    for l in fp.readlines():\n",
        "        proj, loc = l.strip().split('\\t')\n",
        "        if '-' in loc:\n",
        "            start_loc, end_loc = loc.split('-')\n",
        "            start_loc = int(start_loc)\n",
        "            end_loc = int(end_loc)\n",
        "        else:\n",
        "            start_loc, end_loc = int(loc), int(loc) + 1\n",
        "        meta.append([\n",
        "            proj, str(start_loc), str(end_loc)\n",
        "        ])\n",
        "    return meta\n",
        "\n",
        "\n",
        "def read_hypo(hypo_path):\n",
        "    fp = codecs.open(hypo_path, 'r', 'utf-8')\n",
        "    hypo = {}\n",
        "    for l in fp.readlines():\n",
        "        l = l.strip().split()\n",
        "        if l[0][:2] == 'S-':\n",
        "            id = int(l[0][2:])\n",
        "            src = ' '.join(l[1:]).strip()\n",
        "            src = src.replace('@@ ', '')\n",
        "            hypo[id] = {'src': src, 'patches': []}\n",
        "        if l[0][:2] == 'H-':\n",
        "            id = int(l[0][2:])\n",
        "            patch = ' '.join(l[2:]).strip()\n",
        "            patch = patch.replace('@@ ', '')\n",
        "            score = float(l[1])\n",
        "            hypo[id]['patches'].append([patch, score])\n",
        "    return hypo\n",
        "\n",
        "\n",
        "def cure_rerank(meta, hypo_path_list, output_path):\n",
        "    # the patch with same rank from different models are grouped together\n",
        "    group_by_rank = {}\n",
        "    for hypo_path in hypo_path_list:\n",
        "        hypo = read_hypo(hypo_path)\n",
        "        print('finish loading', hypo_path)\n",
        "        for id in hypo:\n",
        "            if id not in group_by_rank:\n",
        "                group_by_rank[id] = {'src': hypo[id]['src'], 'patches': []}\n",
        "            for rank, (patch, score) in enumerate(hypo[id]['patches']):\n",
        "                if rank >= len(group_by_rank[id]['patches']):\n",
        "                    group_by_rank[id]['patches'].append([])\n",
        "                group_by_rank[id]['patches'][rank].append([patch, score])\n",
        "\n",
        "    # the patch with same rank are ranked by scores\n",
        "    reranked_hypo = {}\n",
        "    print('start ranking')\n",
        "    for id in group_by_rank:\n",
        "        key = '-'.join(meta[id])\n",
        "        reranked_hypo[key] = {'src': group_by_rank[id]['src'], 'patches': []}\n",
        "\n",
        "        added_patches = set()\n",
        "        for patches_same_rank in group_by_rank[id]['patches']:\n",
        "            ranked_by_score = sorted(patches_same_rank, key=lambda e: e[1], reverse=True)\n",
        "            for patch, score in ranked_by_score:\n",
        "                if patch not in added_patches:\n",
        "                    added_patches.add(patch)\n",
        "                    reranked_hypo[key]['patches'].append({'patch': patch, 'score': score})\n",
        "            if len(added_patches) >= 5000:\n",
        "                break\n",
        "\n",
        "    print('dumping result in json file')\n",
        "    json.dump(reranked_hypo, open(output_path, 'w'), indent=2)"
      ],
      "metadata": {
        "id": "aKLfGEotsaej"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta_path = RERANK_DIR + '/candidate_patches/QuixBugs/meta.txt'\n",
        "quixbugs_meta = read_quixbugs_meta(meta_path)\n",
        "hypo_path_list = [RERANK_DIR + '/data/patches/gpt_conut_1.txt', RERANK_DIR + '/data/patches/gpt_fconv_1.txt']\n",
        "output_path = RERANK_DIR + '/data/patches/reranked_patches.json'\n",
        "cure_rerank(quixbugs_meta, hypo_path_list, output_path)"
      ],
      "metadata": {
        "id": "DNqcXk0vswky",
        "outputId": "f9496474-8e75-4ff5-8ac4-54142e550974",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finish loading /content/data/patches/gpt_conut_1.txt\n",
            "finish loading /content/data/patches/gpt_fconv_1.txt\n",
            "start ranking\n",
            "dumping result in json file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "VALIDATE_QUIXBUGS_DIR = os.path.abspath('/content')\n",
        "sys.path.append(VALIDATE_QUIXBUGS_DIR + '/src/dataloader/')\n",
        "\n",
        "import tokenization\n",
        "\n",
        "\n",
        "def command_with_timeout(cmd, timeout=5):\n",
        "    p = subprocess.Popen(cmd, stderr=subprocess.PIPE, stdout=subprocess.PIPE, universal_newlines=True)\n",
        "    t_beginning = time.time()\n",
        "    while True:\n",
        "        if p.poll() is not None:\n",
        "            break\n",
        "        seconds_passed = time.time() - t_beginning\n",
        "        if timeout and seconds_passed > timeout:\n",
        "            p.terminate()\n",
        "            return 'TIMEOUT', 'TIMEOUT'\n",
        "        time.sleep(1)\n",
        "    out, err = p.communicate()\n",
        "    return out, err\n",
        "\n",
        "\n",
        "def compile_fix(filename, tmp_dir):\n",
        "    FNULL = open(os.devnull, 'w')\n",
        "    p = subprocess.call([\"javac\",\n",
        "                         tmp_dir + \"Node.java\",\n",
        "                         tmp_dir + \"WeightedEdge.java\",\n",
        "                         filename], stderr=FNULL)\n",
        "    return False if p else True\n",
        "\n",
        "\n",
        "def quixbugs_test_suite(algo, quixbugs_dir):\n",
        "    QUIXBUGS_MAIN_DIR = quixbugs_dir\n",
        "    CUR_DIR = os.getcwd()\n",
        "    FNULL = open(os.devnull, 'w')\n",
        "    try:\n",
        "        os.chdir(QUIXBUGS_MAIN_DIR)\n",
        "        p1 = subprocess.Popen([\"/usr/bin/javac\", \"-cp\", \".:java_programs:junit4-4.12.jar:hamcrest-all-1.3.jar\", \n",
        "                                \"java_testcases/junit/\" + algo.upper() + \"_TEST.java\"],\n",
        "                                stdout=subprocess.PIPE, stderr=FNULL, universal_newlines=True)\n",
        "        out, err = command_with_timeout(\n",
        "            [\"/usr/bin/java\", \"-cp\", \".:java_programs:junit4-4.12.jar:hamcrest-all-1.3.jar\",\n",
        "             \"org.junit.runner.JUnitCore\", \"java_testcases.junit.\" + algo.upper() + \"_TEST\"], timeout=5\n",
        "        )\n",
        "\n",
        "        os.chdir(CUR_DIR)\n",
        "        if \"FAILURES\" in str(out) or \"FAILURES\" in str(err):\n",
        "            return 'wrong'\n",
        "        elif \"TIMEOUT\" in str(out) or \"TIMEOUT\" in str(err):\n",
        "            return 'timeout'\n",
        "        else:\n",
        "            return 'plausible'\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        os.chdir(CUR_DIR)\n",
        "        return 'uncompilable'\n",
        "\n",
        "\n",
        "def insert_fix_quixbugs(file_path, start_loc, end_loc, patch):\n",
        "    shutil.copyfile(file_path, file_path + '.bak')\n",
        "\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = file.readlines()\n",
        "\n",
        "    patched = False\n",
        "    with open(file_path, 'w') as file:\n",
        "        for idx, line in enumerate(data):\n",
        "            if start_loc - 1 <= idx < end_loc - 1:\n",
        "                if not patched:\n",
        "                    file.write(patch)\n",
        "                    patched = True\n",
        "            else:\n",
        "                file.write(line)\n",
        "\n",
        "    return file_path + '.bak'\n",
        "\n",
        "\n",
        "def get_strings_numbers(file_path, loc):\n",
        "    numbers_set = {}\n",
        "    strings_set = {}\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = file.readlines()\n",
        "        for idx, line in enumerate(data):\n",
        "            dist = loc - idx - 1\n",
        "            strings, numbers = tokenization.get_strings_numbers(line)\n",
        "            for num in numbers:\n",
        "                if num != '0' and num != '1':\n",
        "                    if num in numbers_set:\n",
        "                        numbers_set[num] = min(dist, numbers_set[num])\n",
        "                    else:\n",
        "                        numbers_set[num] = dist\n",
        "            for str in strings:\n",
        "                if str in strings_set:\n",
        "                    strings_set[str] = min(dist, strings_set[str])\n",
        "                else:\n",
        "                    strings_set[str] = dist\n",
        "    final_strings = []\n",
        "    final_numbers = []\n",
        "    for k, v in numbers_set.items():\n",
        "        final_numbers.append([k, v])\n",
        "    for k, v in strings_set.items():\n",
        "        final_strings.append([k, v])\n",
        "    final_numbers.sort(key=lambda x: x[1])\n",
        "    final_strings.sort(key=lambda x: x[1])\n",
        "    return final_strings, final_numbers\n",
        "\n",
        "\n",
        "cnt, right = 0, 0\n",
        "\n",
        "\n",
        "def validate_quixbugs(reranked_result_path, output_path, tmp_dir):\n",
        "    global cnt, right\n",
        "\n",
        "    if not os.path.exists(tmp_dir):\n",
        "        command_with_timeout(['mkdir', tmp_dir])\n",
        "\n",
        "    reranked_result = json.load(open(reranked_result_path, 'r'))\n",
        "    validated_result = {}\n",
        "    for key in reranked_result:\n",
        "        cnt += 1\n",
        "\n",
        "        proj, start_loc, end_loc = key.split('-')\n",
        "        print(right, '/', cnt, proj)\n",
        "\n",
        "        command_with_timeout(['rm', '-rf', tmp_dir + '/java_programs/'])\n",
        "        command_with_timeout(['mkdir', tmp_dir + '/java_programs/'])\n",
        "\n",
        "        shutil.copyfile(tmp_dir + \"/java_programs_bak/\" + proj + '.java',\n",
        "                        tmp_dir + \"/java_programs/\" + proj + '.java')\n",
        "        shutil.copyfile(tmp_dir + \"/java_programs_bak/Node.java\", tmp_dir + \"/java_programs/Node.java\")\n",
        "        shutil.copyfile(tmp_dir + \"/java_programs_bak/WeightedEdge.java\", tmp_dir + \"/java_programs/WeightedEdge.java\")\n",
        "\n",
        "        validated_result[key] = {'src': reranked_result[key]['src'], 'patches': []}\n",
        "        bug_start_time = time.time()\n",
        "        current_is_correct = False\n",
        "        for tokenized_patch in reranked_result[key]['patches']:\n",
        "            # validate 5 hours for each bug at most\n",
        "            if time.time() - bug_start_time > 5 * 3600:\n",
        "                break\n",
        "            # validate 5000 patches for each bug at most\n",
        "            if len(validated_result[key]['patches']) >= 5000:\n",
        "                break\n",
        "            filename = tmp_dir + \"/java_programs/\" + proj + '.java'\n",
        "\n",
        "            score = tokenized_patch['score']\n",
        "            tokenized_patch = tokenized_patch['patch']\n",
        "\n",
        "            strings, numbers = get_strings_numbers(filename, (int(start_loc) + int(end_loc)) // 2)\n",
        "            strings = [item[0] for item in strings][:5]\n",
        "            numbers = [item[0] for item in numbers][:5]\n",
        "            # one tokenized patch may be reconstructed to multiple source-code patches\n",
        "            reconstructed_patches = tokenization.token2statement(tokenized_patch.split(' '), numbers, strings)\n",
        "            # validate most 5 source-code patches come from the same tokenized patch\n",
        "            for patch in reconstructed_patches[:5]:\n",
        "                patch = patch.strip()\n",
        "                insert_fix_quixbugs(filename, int(start_loc), int(end_loc), patch)\n",
        "                compile = compile_fix(filename, tmp_dir + \"/java_programs/\")\n",
        "                correctness = 'uncompilable'\n",
        "                if compile:\n",
        "                    correctness = quixbugs_test_suite(proj, quixbugs_dir=tmp_dir)\n",
        "                    if correctness == 'plausible':\n",
        "                        if not current_is_correct:\n",
        "                            right += 1\n",
        "                            current_is_correct = True\n",
        "                        print(right, '/', cnt, \"Plausible patch:\", patch)\n",
        "                        break\n",
        "                    elif correctness == 'wrong':\n",
        "                        print(right, '/', cnt, \"Wrong patch:\", patch)\n",
        "                    elif correctness == 'timeout':\n",
        "                        print(right, '/', cnt, \"Timeout patch:\", patch)\n",
        "                else:\n",
        "                    print(right, '/', cnt, 'Uncompilable patch:', patch)\n",
        "                validated_result[key]['patches'].append({\n",
        "                    'patch': patch, 'correctness': correctness\n",
        "                })\n",
        "                shutil.copyfile(tmp_dir + \"/java_programs_bak/\" + proj + '.java',\n",
        "                                tmp_dir + \"/java_programs/\" + proj + '.java')\n",
        "            json.dump(validated_result, open(output_path, 'w'), indent=2)\n",
        "            if current_is_correct:\n",
        "                break\n",
        "        json.dump(validated_result, open(output_path, 'w'), indent=2)\n"
      ],
      "metadata": {
        "id": "Pcs4aiErvNQp"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    reranked_result_path = VALIDATE_QUIXBUGS_DIR + '/data/patches/reranked_patches.json'\n",
        "    output_path = VALIDATE_QUIXBUGS_DIR + '/data/patches/validated_patches.json'\n",
        "    tmp_dir = VALIDATE_QUIXBUGS_DIR + '/tmp/validate_quixbugs'\n",
        "    validate_quixbugs(reranked_result_path, output_path, tmp_dir)"
      ],
      "metadata": {
        "id": "8IIts59WvsBH",
        "outputId": "15633d35-8929-4703-fa60-f042d3c0c917",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 / 6 BITCOUNT\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-e096d2c4d478>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moutput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVALIDATE_QUIXBUGS_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/data/patches/validated_patches.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtmp_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVALIDATE_QUIXBUGS_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/tmp/validate_quixbugs'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvalidate_quixbugs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreranked_result_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-25cdef09e2c2>\u001b[0m in \u001b[0;36mvalidate_quixbugs\u001b[0;34m(reranked_result_path, output_path, tmp_dir)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         shutil.copyfile(tmp_dir + \"/java_programs_bak/\" + proj + '.java',\n\u001b[0;32m--> 135\u001b[0;31m                         tmp_dir + \"/java_programs/\" + proj + '.java')\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/java_programs_bak/Node.java\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/java_programs/Node.java\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/java_programs_bak/WeightedEdge.java\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/java_programs/WeightedEdge.java\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/tmp/validate_quixbugs/java_programs_bak/BITCOUNT.java'"
          ]
        }
      ]
    }
  ]
}