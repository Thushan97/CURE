{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNc54yBc46yTVtyoh50mhEw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thushan97/CURE/blob/master/cure.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04b5Y2KlVvaJ",
        "outputId": "4cdaf8de-c93b-4da9-b6bd-0f9823924b15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CURE'...\n",
            "remote: Enumerating objects: 990, done.\u001b[K\n",
            "remote: Counting objects: 100% (318/318), done.\u001b[K\n",
            "remote: Compressing objects: 100% (275/275), done.\u001b[K\n",
            "remote: Total 990 (delta 41), reused 295 (delta 29), pack-reused 672\u001b[K\n",
            "Receiving objects: 100% (990/990), 79.61 MiB | 12.86 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Thushan97/CURE.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/CURE/* /content/"
      ],
      "metadata": {
        "id": "Lf4LBkfjV9pW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pretrain model download\n",
        "!wget \"https://zenodo.org/record/7030145/files/models.tar.xz?download=1\" -c -O 'models.tar.xz'\n",
        "!mkdir /content/data/models\n",
        "!tar -xf models.tar.xz\n",
        "!mv /content/models/* /content/data/models/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yS6kXFIVXO9J",
        "outputId": "5250a323-bca5-466b-8023-1950e2aa2e40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-12 12:40:41--  https://zenodo.org/record/7030145/files/models.tar.xz?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.184.117.155\n",
            "Connecting to zenodo.org (zenodo.org)|188.184.117.155|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1911937724 (1.8G) [application/octet-stream]\n",
            "Saving to: ‘models.tar.xz’\n",
            "\n",
            "models.tar.xz       100%[===================>]   1.78G  15.7MB/s    in 2m 2s   \n",
            "\n",
            "2022-09-12 12:42:46 (14.9 MB/s) - ‘models.tar.xz’ saved [1911937724/1911937724]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==2.10.0 subword-nmt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujyExeoEZ6VM",
        "outputId": "3debcc46-47b2-44a0-b682-4273fe0712be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==2.10.0\n",
            "  Downloading transformers-2.10.0-py3-none-any.whl (660 kB)\n",
            "\u001b[K     |████████████████████████████████| 660 kB 33.1 MB/s \n",
            "\u001b[?25hCollecting subword-nmt\n",
            "  Downloading subword_nmt-0.3.8-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.10.0) (3.8.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.10.0) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.10.0) (2022.6.2)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 51.0 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 61.8 MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.7.0\n",
            "  Downloading tokenizers-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 31.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.10.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.10.0) (1.21.6)\n",
            "Collecting mock\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.10.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.10.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.10.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.10.0) (2022.6.15)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.10.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.10.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.10.0) (1.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=ebcb4a7757d4d19c2f5fb23919e4c0060f23a88babd7f44fd00ac5f9f77645f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, mock, transformers, subword-nmt\n",
            "Successfully installed mock-4.0.3 sacremoses-0.0.53 sentencepiece-0.1.97 subword-nmt-0.3.8 tokenizers-0.7.0 transformers-2.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "OiM2X0qPWds0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run gpt_conut_trainer.py file"
      ],
      "metadata": {
        "id": "9NPPvZ58sqY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import codecs\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import OpenAIGPTLMHeadModel\n",
        "\n",
        "GPT_CONUT_TRAINER_DIR = os.path.abspath('/content')#os.path.abspath(__file__)[: os.path.abspath(__file__).rindex('/') + 1]"
      ],
      "metadata": {
        "id": "YNQLj3YJWdTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from src.models.gpt_conut import GPTCoNuTModel\n",
        "from src.dataloader.dictionary import Dictionary\n",
        "from src.dataloader.gpt_conut_data_loader import GPTCoNuTDataLoader"
      ],
      "metadata": {
        "id": "Ff_GgGKJWWyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "DyuHj06gvReD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f'CUDA GPU availible : {torch.cuda.is_available()}')\n",
        "# DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "VpeQL1nBWybZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTCoNuTTrainer():\n",
        "    def __init__(self, train_loader, valid_loader, dictionary, gpt_file):\n",
        "        gpt_loaded = torch.load(gpt_file)\n",
        "        config = gpt_loaded['config']\n",
        "        gpt_model = OpenAIGPTLMHeadModel(config).cuda()\n",
        "        gpt_model.load_state_dict(gpt_loaded['model'])\n",
        "\n",
        "        self.train_loader = train_loader\n",
        "        self.valid_loader = valid_loader\n",
        "        self.dictionary = dictionary\n",
        "\n",
        "        self.batch_size = 12\n",
        "        self.load_size = 1200   # load 1200 samples from training data every time\n",
        "\n",
        "        self.gpt_model = gpt_model\n",
        "        self.model = None\n",
        "        self.hyper_parameter = {}\n",
        "        self.optimizer = None\n",
        "        self.current_train_step = 0\n",
        "        self.val_loss = {}\n",
        "\n",
        "    def shuffle_dataset(self):\n",
        "        indices = [i for i in range(len(self.train_loader.dataset))]\n",
        "        random.shuffle(indices)\n",
        "        return indices\n",
        "\n",
        "    def train_step(self, samples):\n",
        "        self.model.train()\n",
        "        self.current_train_step += 1\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        batch = self.train_loader.dataset.collater(samples)\n",
        "        if torch.cuda.is_available():\n",
        "            outputs = self.model(\n",
        "                batch['net_input']['src_tokens'].cuda(),\n",
        "                batch['net_input']['src_with_prev_context'].cuda(),\n",
        "                batch['net_input']['ctx_tokens'].cuda(),\n",
        "                prev_tokens_index=batch['target_index'].cuda(),\n",
        "                prev_tokens_with_context=batch['target_with_prev_context'].cuda(),\n",
        "                labels=batch['target'].cuda(),\n",
        "            )\n",
        "        else:\n",
        "            outputs = self.model(\n",
        "                batch['net_input']['src_tokens'],\n",
        "                batch['net_input']['src_with_prev_context'],\n",
        "                batch['net_input']['ctx_tokens'],\n",
        "                prev_tokens_index=batch['target_index'],\n",
        "                prev_tokens_with_context=batch['target_with_prev_context'],\n",
        "                labels=batch['target'],\n",
        "            )\n",
        "        logits, avg_attn_scores, apr_loss, lm_loss = outputs[:4]\n",
        "        loss = apr_loss + 0.3 * lm_loss\n",
        "        loss.mean().backward()\n",
        "        nn.utils.clip_grad_norm_(self.model.parameters(), 0.5, norm_type=2)\n",
        "        self.optimizer.step()\n",
        "        return loss.mean().item(), apr_loss.mean().item(), lm_loss.mean().item()\n",
        "\n",
        "    def valid_step(self, samples):\n",
        "        self.model.eval()\n",
        "        batch = self.valid_loader.dataset.collater(samples)\n",
        "        outputs = self.model(\n",
        "            batch['net_input']['src_tokens'].cuda(),\n",
        "            batch['net_input']['src_with_prev_context'].cuda(),\n",
        "            batch['net_input']['ctx_tokens'].cuda(),\n",
        "            prev_tokens_index=batch['target_index'].cuda(),\n",
        "            prev_tokens_with_context=batch['target_with_prev_context'].cuda(),\n",
        "            labels=batch['target'].cuda(),\n",
        "        )\n",
        "        logits, avg_attn_scores, apr_loss, lm_loss = outputs[:4]\n",
        "        loss = apr_loss + 0.3 * lm_loss\n",
        "        return loss.mean().item(), apr_loss.mean().item(), lm_loss.mean().item(), logits\n",
        "\n",
        "    def validate_and_save(self, model_id, save_dir):\n",
        "        oom = 0\n",
        "        with torch.no_grad():\n",
        "            val_loss, val_fconv_loss, val_lm_loss = [], [], []\n",
        "            for i in range(0, self.valid_loader.total_size, self.batch_size):\n",
        "                samples = [self.valid_loader.dataset[j]\n",
        "                           for j in range(i, min(len(self.valid_loader.dataset), i + self.batch_size))]\n",
        "                try:\n",
        "                    loss, fconv_loss, lm_loss, logits = self.valid_step(samples)\n",
        "                    val_loss.append(float(loss))\n",
        "                    val_fconv_loss.append(float(fconv_loss))\n",
        "                    val_lm_loss.append(float(lm_loss))\n",
        "                except Exception as e:\n",
        "                    oom += 1\n",
        "\n",
        "            info = 'val loss:{}, val apr_loss:{}, val lm_loss:{}, val ppl:{}, oom:{}'.format(\n",
        "                round(float(np.mean(val_loss)), 6),\n",
        "                round(float(np.mean(val_fconv_loss)), 6),\n",
        "                round(float(np.mean(val_lm_loss)), 6),\n",
        "                round(float(np.exp(np.mean(val_loss))), 6),\n",
        "                oom\n",
        "            )\n",
        "            print(info)\n",
        "\n",
        "            val_loss = np.mean(val_fconv_loss)\n",
        "            checkpoint = {\n",
        "                'model': self.model.state_dict(),\n",
        "                'optimizer': self.optimizer.state_dict(),\n",
        "                'current_step': self.current_train_step,\n",
        "                # 'config': self.model.module.config(),\n",
        "                'val_loss': val_loss,\n",
        "            }\n",
        "            torch.save(checkpoint, save_dir + 'gpt_conut_' + str(model_id) + '.pt')\n",
        "            self.val_loss[model_id] = {\n",
        "                'val_loss': val_loss,\n",
        "                'hyper-parameter': str(self.hyper_parameter),\n",
        "            }\n",
        "\n",
        "        return val_loss\n",
        "\n",
        "    def train(self, model_id, epochs, hyper_parameter, save_dir):\n",
        "        self.hyper_parameter = hyper_parameter\n",
        "        self.model = GPTCoNuTModel(\n",
        "            self.dictionary, embed_dim=384, max_positions=1024,\n",
        "            src_encoder_convolutions=self.hyper_parameter['src_encoder_convolutions'],\n",
        "            ctx_encoder_convolutions=self.hyper_parameter['ctx_encoder_convolutions'],\n",
        "            decoder_convolutions=self.hyper_parameter['decoder_convolutions'],\n",
        "            dropout=self.hyper_parameter['dropout'], embed_model=self.gpt_model,\n",
        "        ).cuda()\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=6.25e-5)\n",
        "        # self.model = nn.DataParallel(self.model, device_ids=device_ids)\n",
        "        \n",
        "        self.valid_loader.load_data(0, self.valid_loader.total_size)\n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            for i in range(0, self.train_loader.total_size, self.load_size):\n",
        "                oom = 0\n",
        "                self.train_loader.load_data(i, i + self.load_size)\n",
        "                indices = self.shuffle_dataset()\n",
        "                train_loss, train_apr_loss, train_lm_loss = [], [], []\n",
        "\n",
        "                start, end = 0, 0\n",
        "                samples = []\n",
        "                max_src, max_ctx, max_tgt = 0, 0, 0\n",
        "                while end < len(self.train_loader.dataset):\n",
        "                    sample = self.train_loader.dataset[indices[end]]\n",
        "                    if max_ctx + len(sample['target']) >= 1023 \\\n",
        "                            or max_tgt + len(sample['prev_context']) >= 1023 \\\n",
        "                            or max_ctx + len(sample['source']) >= 1023 \\\n",
        "                            or max_src + len(sample['prev_context']) >= 1023 \\\n",
        "                            or end - start == self.batch_size:\n",
        "                        try:\n",
        "                            loss, apr_loss, lm_loss = self.train_step(samples)\n",
        "                            train_loss.append(loss)\n",
        "                            train_apr_loss.append(apr_loss)\n",
        "                            train_lm_loss.append(lm_loss)\n",
        "                        except Exception as e:\n",
        "                            oom += 1\n",
        "\n",
        "                        start = end\n",
        "                        max_src, max_ctx, max_tgt = 0, 0, 0\n",
        "                        samples = []\n",
        "                        continue\n",
        "                    max_src = max(max_src, len(sample['source']))\n",
        "                    max_ctx = max(max_ctx, len(sample['prev_context']))\n",
        "                    max_tgt = max(max_tgt, len(sample['target']))\n",
        "                    end += 1\n",
        "                    samples.append(sample)\n",
        "                if len(samples) > 0:\n",
        "                    try:\n",
        "                        loss, apr_loss, lm_loss = self.train_step(samples)\n",
        "                        train_loss.append(loss)\n",
        "                        train_apr_loss.append(apr_loss)\n",
        "                        train_lm_loss.append(lm_loss)\n",
        "                    except Exception as e:\n",
        "                        oom += 1\n",
        "\n",
        "                if (i // self.load_size) % 10 == 0:\n",
        "                    info = 'epoch:{}, load data:{}, lr:{}, loss:{}, apr_loss:{}, lm_loss:{}, time:{}s, oom:{}'.\\\n",
        "                        format(epoch + 1, i + self.load_size,\n",
        "                               round(self.optimizer.param_groups[0]['lr'], 10),\n",
        "                               round(float(np.mean(train_loss)), 6),\n",
        "                               round(float(np.mean(train_apr_loss)), 6),\n",
        "                               round(float(np.mean(train_lm_loss)), 6),\n",
        "                               int(time.time() - start_time), oom\n",
        "                               )\n",
        "                    start_time = time.time()\n",
        "                    print(str(model_id) + ' ' + info)\n",
        "\n",
        "                if (i // self.load_size) % 100 == 0:\n",
        "                    self.validate_and_save(model_id, save_dir)\n",
        "        self.validate_and_save(model_id, save_dir)"
      ],
      "metadata": {
        "id": "KffOeTnGW1bG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    device_ids = [0, 1, 2, 3]\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
        "    \n",
        "    vocab_file = GPT_CONUT_TRAINER_DIR + '/data/vocabulary/vocabulary.txt'\n",
        "    train_file = GPT_CONUT_TRAINER_DIR + '/data/data/training_bpe.txt'\n",
        "    valid_file = GPT_CONUT_TRAINER_DIR + '/data/data/validation_bpe.txt'\n",
        "    gpt_file = GPT_CONUT_TRAINER_DIR + '/data/models/code_gpt.pt'\n",
        "\n",
        "    dictionary = Dictionary(vocab_file, min_cnt=0)\n",
        "    print('dictionary initialized, vocab size:{}'.format(len(dictionary)))\n",
        "\n",
        "    train_loader = GPTCoNuTDataLoader(train_file, dictionary)\n",
        "    valid_loader = GPTCoNuTDataLoader(valid_file, dictionary)\n",
        "    print('data loader initialized, train size:{}, validate size:{}'.\n",
        "          format(train_loader.total_size, valid_loader.total_size))\n",
        "\n",
        "    trainer = GPTCoNuTTrainer(train_loader, valid_loader, dictionary, gpt_file)\n",
        "\n",
        "    hyper_parameter = {\n",
        "        'src_encoder_convolutions': ((192, 5),) * 1,\n",
        "        'ctx_encoder_convolutions': ((384, 5),) * 1,\n",
        "        'decoder_convolutions': ((192, 5),) * 1,\n",
        "        'dropout': 0.1,\n",
        "    }\n",
        "    model_id = 1\n",
        "    epochs = 5\n",
        "    trainer.train(model_id, epochs, hyper_parameter, save_dir=GPT_CONUT_TRAINER_DIR + '/data/models/')"
      ],
      "metadata": {
        "id": "7doawc2lW8o5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f228b00-7eab-42e2-cdc6-62ea81b7eff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dictionary initialized, vocab size:50061\n",
            "data loader initialized, train size:2000, validate size:100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 epoch:1, load data:1200, lr:6.25e-05, loss:9.26268, apr_loss:6.878682, lm_loss:7.946659, time:63s, oom:36\n",
            "val loss:5.058121, val apr_loss:3.304267, val lm_loss:5.846182, val ppl:157.294747, oom:0\n",
            "1 epoch:2, load data:1200, lr:6.25e-05, loss:3.467829, apr_loss:3.062547, lm_loss:1.350938, time:66s, oom:34\n",
            "val loss:2.32264, val apr_loss:2.110734, val lm_loss:0.706351, val ppl:10.202569, oom:0\n",
            "1 epoch:3, load data:1200, lr:6.25e-05, loss:2.268841, apr_loss:2.029103, lm_loss:0.799127, time:66s, oom:36\n",
            "val loss:1.682003, val apr_loss:1.480034, val lm_loss:0.67323, val ppl:5.376316, oom:0\n",
            "1 epoch:4, load data:1200, lr:6.25e-05, loss:1.770133, apr_loss:1.547027, lm_loss:0.743687, time:70s, oom:30\n",
            "val loss:1.404391, val apr_loss:1.20386, val lm_loss:0.668437, val ppl:4.073047, oom:0\n",
            "1 epoch:5, load data:1200, lr:6.25e-05, loss:1.574451, apr_loss:1.348321, lm_loss:0.753765, time:68s, oom:33\n",
            "val loss:1.270049, val apr_loss:1.073574, val lm_loss:0.654915, val ppl:3.561027, oom:0\n",
            "val loss:1.233361, val apr_loss:1.036462, val lm_loss:0.656332, val ppl:3.432748, oom:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run gpt_fconv_trainer.py file"
      ],
      "metadata": {
        "id": "hDGLIYMss1kL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import codecs\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import OpenAIGPTLMHeadModel\n",
        "\n",
        "# GPT_FCONV_TRAINER_DIR = os.path.abspath(__file__)[: os.path.abspath(__file__).rindex('/') + 1]\n",
        "GPT_FCONV_TRAINER_DIR = os.path.abspath('/content')\n",
        "\n",
        "from src.models.gpt_fconv import GPTFConvModel\n",
        "from src.dataloader.dictionary import Dictionary\n",
        "from src.dataloader.gpt_fconv_data_loader import GPTFConvDataLoader\n",
        "\n",
        "\n",
        "class GPTFConvTrainer():\n",
        "    def __init__(self, train_loader, valid_loader, dictionary, gpt_file):\n",
        "        gpt_loaded = torch.load(gpt_file)\n",
        "        config = gpt_loaded['config']\n",
        "        gpt_model = OpenAIGPTLMHeadModel(config).cuda()\n",
        "        gpt_model.load_state_dict(gpt_loaded['model'])\n",
        "\n",
        "        self.train_loader = train_loader\n",
        "        self.valid_loader = valid_loader\n",
        "        self.dictionary = dictionary\n",
        "\n",
        "        self.batch_size = 12\n",
        "        self.load_size = 1200\n",
        "\n",
        "        self.gpt_model = gpt_model\n",
        "        self.model = None\n",
        "        self.hyper_parameter = {}\n",
        "        self.hyper_parameter_set = {'{}'}\n",
        "        self.optimizer = None\n",
        "        self.current_train_step = 0\n",
        "        self.val_loss = {}\n",
        "\n",
        "    def shuffle_dataset(self):\n",
        "        indices = [i for i in range(len(self.train_loader.dataset))]\n",
        "        random.shuffle(indices)\n",
        "        return indices\n",
        "\n",
        "    def train_step(self, samples):\n",
        "        self.model.train()\n",
        "        self.current_train_step += 1\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        batch = self.train_loader.dataset.collater(samples)\n",
        "        if torch.cuda.is_available():\n",
        "            outputs = self.model(\n",
        "                batch['net_input']['src_tokens'].cuda(),\n",
        "                batch['net_input']['src_with_prev_context'].cuda(),\n",
        "                prev_tokens_index=batch['target_index'].cuda(),\n",
        "                prev_tokens_with_context=batch['target_with_prev_context'].cuda(),\n",
        "                labels=batch['target'].cuda(),\n",
        "            )\n",
        "\n",
        "        logits, avg_attn_scores, apr_loss, lm_loss = outputs[:4]\n",
        "        loss = apr_loss + 0.3 * lm_loss\n",
        "        loss.mean().backward()\n",
        "        nn.utils.clip_grad_norm_(self.model.parameters(), 0.5, norm_type=2)\n",
        "        self.optimizer.step()\n",
        "        return loss.mean().item(), apr_loss.mean().item(), lm_loss.mean().item()\n",
        "\n",
        "    def valid_step(self, samples):\n",
        "        self.model.eval()\n",
        "        batch = self.valid_loader.dataset.collater(samples)\n",
        "        outputs = self.model(\n",
        "            batch['net_input']['src_tokens'].cuda(),\n",
        "            batch['net_input']['src_with_prev_context'].cuda(),\n",
        "            prev_tokens_index=batch['target_index'].cuda(),\n",
        "            prev_tokens_with_context=batch['target_with_prev_context'].cuda(),\n",
        "            labels=batch['target'].cuda(),\n",
        "        )\n",
        "        logits, avg_attn_scores, apr_loss, lm_loss = outputs[:4]\n",
        "        loss = apr_loss + 0.3 * lm_loss\n",
        "        return loss.mean().item(), apr_loss.mean().item(), lm_loss.mean().item(), logits\n",
        "\n",
        "    def validate_and_save(self, model_id, save_dir):\n",
        "        oom = 0\n",
        "        with torch.no_grad():\n",
        "            val_loss, val_fconv_loss, val_lm_loss = [], [], []\n",
        "            for i in range(0, self.valid_loader.total_size, self.batch_size):\n",
        "                samples = [self.valid_loader.dataset[j]\n",
        "                           for j in range(i, min(len(self.valid_loader.dataset), i + self.batch_size))]\n",
        "                try:\n",
        "                    loss, fconv_loss, lm_loss, logits = self.valid_step(samples)\n",
        "                    val_loss.append(float(loss))\n",
        "                    val_fconv_loss.append(float(fconv_loss))\n",
        "                    val_lm_loss.append(float(lm_loss))\n",
        "                except Exception as e:\n",
        "                    oom += 1\n",
        "\n",
        "            info = 'val loss:{}, val apr_loss:{}, val lm_loss:{}, val ppl:{}, oom:{}'.format(\n",
        "                round(float(np.mean(val_loss)), 6),\n",
        "                round(float(np.mean(val_fconv_loss)), 6),\n",
        "                round(float(np.mean(val_lm_loss)), 6),\n",
        "                round(float(np.exp(np.mean(val_loss))), 6),\n",
        "                oom\n",
        "            )\n",
        "            print(info)\n",
        "\n",
        "            val_loss = np.mean(val_fconv_loss)\n",
        "            checkpoint = {\n",
        "                'model': self.model.state_dict(),\n",
        "                'optimizer': self.optimizer.state_dict(),\n",
        "                'current_step': self.current_train_step,\n",
        "                'config': self.model.config(),\n",
        "                'val_loss': val_loss,\n",
        "            }\n",
        "            torch.save(checkpoint, save_dir + 'gpt_fconv_' + str(model_id) + '.pt')\n",
        "            self.val_loss[model_id] = {\n",
        "                'val_loss': val_loss,\n",
        "                'hyper-parameter': str(self.hyper_parameter),\n",
        "            }\n",
        "        return val_loss\n",
        "\n",
        "    def train(self, model_id, epochs, hyper_parameter, save_dir):\n",
        "        self.hyper_parameter = hyper_parameter\n",
        "        self.model = GPTFConvModel(\n",
        "                self.dictionary, embed_dim=384, max_positions=1024,\n",
        "                encoder_convolutions=self.hyper_parameter['encoder_convolutions'],\n",
        "                decoder_convolutions=self.hyper_parameter['decoder_convolutions'],\n",
        "                dropout=self.hyper_parameter['dropout'], embed_model=self.gpt_model,\n",
        "            ).cuda()\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=6.25e-5)\n",
        "        # self.model = nn.DataParallel(self.model, device_ids=device_ids)\n",
        "        \n",
        "        self.valid_loader.load_data(0, self.valid_loader.total_size)\n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            for i in range(0, self.train_loader.total_size, self.load_size):\n",
        "                oom = 0\n",
        "                self.train_loader.load_data(i, i + self.load_size)\n",
        "                indices = self.shuffle_dataset()\n",
        "                train_loss, train_apr_loss, train_lm_loss = [], [], []\n",
        "\n",
        "                start, end = 0, 0\n",
        "                samples = []\n",
        "                max_src, max_ctx, max_tgt = 0, 0, 0\n",
        "                while end < len(self.train_loader.dataset):\n",
        "                    sample = self.train_loader.dataset[indices[end]]\n",
        "                    if max_ctx + len(sample['target']) >= 1023 \\\n",
        "                            or max_tgt + len(sample['prev_context']) >= 1023 \\\n",
        "                            or max_ctx + len(sample['source']) >= 1023 \\\n",
        "                            or max_src + len(sample['prev_context']) >= 1023 \\\n",
        "                            or end - start == self.batch_size:\n",
        "                        try:\n",
        "                            loss, apr_loss, lm_loss = self.train_step(samples)\n",
        "                            train_loss.append(loss)\n",
        "                            train_apr_loss.append(apr_loss)\n",
        "                            train_lm_loss.append(lm_loss)\n",
        "                        except Exception as e:\n",
        "                            oom += 1\n",
        "\n",
        "                        start = end\n",
        "                        max_src, max_ctx, max_tgt = 0, 0, 0\n",
        "                        samples = []\n",
        "                        continue\n",
        "                    max_src = max(max_src, len(sample['source']))\n",
        "                    max_ctx = max(max_ctx, len(sample['prev_context']))\n",
        "                    max_tgt = max(max_tgt, len(sample['target']))\n",
        "                    end += 1\n",
        "                    samples.append(sample)\n",
        "                if len(samples) > 0:\n",
        "                    try:\n",
        "                        loss, apr_loss, lm_loss = self.train_step(samples)\n",
        "                        train_loss.append(loss)\n",
        "                        train_apr_loss.append(apr_loss)\n",
        "                        train_lm_loss.append(lm_loss)\n",
        "                    except Exception as e:\n",
        "                        oom += 1\n",
        "\n",
        "                if (i // self.load_size) % 10 == 0:\n",
        "                    info = 'epoch:{}, load data:{}, lr:{}, loss:{}, apr_loss:{}, lm_loss:{}, time:{}s, oom:{}'.\\\n",
        "                        format(epoch + 1, i + self.load_size,\n",
        "                               round(self.optimizer.param_groups[0]['lr'], 10),\n",
        "                               round(float(np.mean(train_loss)), 6),\n",
        "                               round(float(np.mean(train_apr_loss)), 6),\n",
        "                               round(float(np.mean(train_lm_loss)), 6),\n",
        "                               int(time.time() - start_time), oom\n",
        "                               )\n",
        "                    start_time = time.time()\n",
        "                    print(str(model_id) + ' ' + info)\n",
        "\n",
        "                if (i // self.load_size) % 100 == 0:\n",
        "                    self.validate_and_save(model_id, save_dir)\n",
        "        self.validate_and_save(model_id, save_dir)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    device_ids = [0, 1, 2, 3]\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
        "    \n",
        "    vocab_file = GPT_FCONV_TRAINER_DIR + '/data/vocabulary/vocabulary.txt'\n",
        "    train_file = GPT_FCONV_TRAINER_DIR + '/data/data/training_bpe.txt'\n",
        "    valid_file = GPT_FCONV_TRAINER_DIR + '/data/data/validation_bpe.txt'\n",
        "    gpt_file = GPT_FCONV_TRAINER_DIR + '/data/models/code_gpt.pt'\n",
        "\n",
        "    dictionary = Dictionary(vocab_file, min_cnt=0)\n",
        "    print('dictionary initialized, vocab size:{}'.format(len(dictionary)))\n",
        "\n",
        "    train_loader = GPTFConvDataLoader(train_file, dictionary)\n",
        "    valid_loader = GPTFConvDataLoader(valid_file, dictionary)\n",
        "    print('data loader initialized, train size:{}, validate size:{}'.\n",
        "          format(train_loader.total_size, valid_loader.total_size))\n",
        "\n",
        "    trainer = GPTFConvTrainer(train_loader, valid_loader, dictionary, gpt_file)\n",
        "\n",
        "    hyper_parameter = {\n",
        "        'encoder_convolutions': ((192, 5),) * 1,\n",
        "        'decoder_convolutions': ((192, 5),) * 1,\n",
        "        'dropout': 0.1,\n",
        "    }\n",
        "    trainer.train(1, 2, hyper_parameter, save_dir=GPT_FCONV_TRAINER_DIR + '/data/models/')\n"
      ],
      "metadata": {
        "id": "JDCQxfDh4pXX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa797917-7140-4ce2-acc3-12039182a363"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dictionary initialized, vocab size:50061\n",
            "data loader initialized, train size:2000, validate size:100\n",
            "1 epoch:1, load data:1200, lr:6.25e-05, loss:7.784297, apr_loss:5.795911, lm_loss:6.627955, time:56s, oom:12\n",
            "val loss:3.369558, val apr_loss:2.734781, val lm_loss:2.115924, val ppl:29.065682, oom:0\n",
            "1 epoch:2, load data:1200, lr:6.25e-05, loss:2.67298, apr_loss:2.404278, lm_loss:0.895674, time:55s, oom:12\n",
            "val loss:1.878901, val apr_loss:1.64997, val lm_loss:0.763105, val ppl:6.546308, oom:0\n",
            "val loss:1.601408, val apr_loss:1.37689, val lm_loss:0.748394, val ppl:4.960009, oom:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run generator.py file"
      ],
      "metadata": {
        "id": "mvFxGHgos-t9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import codecs\n",
        "import torch\n",
        "import sys\n",
        "import os\n",
        "from transformers import OpenAIGPTLMHeadModel\n",
        "\n",
        "# GENERATOR_DIR = os.path.abspath(__file__)[: os.path.abspath(__file__).rindex('/') + 1]\n",
        "GENERATOR_DIR = os.path.abspath('/content')\n",
        "sys.path.append(GENERATOR_DIR + '/models/')\n",
        "sys.path.append(GENERATOR_DIR + '/dataloader/')\n",
        "sys.path.append(GENERATOR_DIR + '/tester/')\n",
        "from src.dataloader.gpt_conut_data_loader import GPTCoNuTDataLoader\n",
        "from src.dataloader.gpt_fconv_data_loader import GPTFConvDataLoader\n",
        "from src.dataloader.identifier_data_loader import IdentifierDataLoader\n",
        "from src.dataloader.dictionary import Dictionary\n",
        "from src.models.gpt_conut import GPTCoNuTModel\n",
        "from src.models.gpt_fconv import GPTFConvModel\n",
        "from src.tester.beamsearch import BeamSearch\n",
        "\n",
        "\n",
        "class Generator():\n",
        "    def __init__(self, model, dictionary, data_loader, beam_size=10):\n",
        "        self.model = model\n",
        "        self.dictionary = dictionary\n",
        "        self.data_loader = data_loader\n",
        "        self.beam_size = beam_size\n",
        "        self.beamsearch = BeamSearch(model, dictionary, beam_size)\n",
        "        print(self.model, beam_size)\n",
        "\n",
        "    def generate(self, output_path):\n",
        "        wp = codecs.open(output_path, 'w', 'utf-8')\n",
        "        self.data_loader.load_data(0, self.data_loader.total_size)\n",
        "        for i in range(self.data_loader.total_size):\n",
        "            print(i, '/', self.data_loader.total_size)\n",
        "            data = self.data_loader.dataset[i]\n",
        "            if True:\n",
        "                self.beamsearch.beam_size = self.beam_size\n",
        "                sample = self.data_loader.dataset.collater([data])\n",
        "                with torch.no_grad():\n",
        "                    if isinstance(self.model, GPTCoNuTModel):\n",
        "                        hypothesis = self.beamsearch.generate_gpt_conut(sample)\n",
        "                    elif isinstance(self.model, GPTFConvModel):\n",
        "                        hypothesis = self.beamsearch.generate_gpt_fconv(sample)\n",
        "            # except Exception as e:\n",
        "            #    print(e)\n",
        "            #    continue\n",
        "            id = str(sample['id'].item())\n",
        "            wp.write('S-{}\\t'.format(id))\n",
        "            wp.write(self.dictionary.string(data['source']) + '\\n')\n",
        "            wp.write('T-{}\\t'.format(id))\n",
        "            wp.write(self.dictionary.string(data['target']) + '\\n')\n",
        "            for h in hypothesis:\n",
        "                wp.write('H-{}\\t{}\\t'.format(id, str(h['final_score'])))\n",
        "                wp.write(self.dictionary.string(h['hypo']) + '\\n')\n",
        "                wp.write('P-{}\\t'.format(id))\n",
        "                wp.write(' '.join(str(round(s.item(), 4)) for s in h['score']) + '\\n')\n",
        "        wp.close()\n",
        "\n",
        "\n",
        "def generate_gpt_conut(vocab_file, model_file, input_file, identifier_txt_file, identifier_token_file, output_file, beam_size):\n",
        "    dictionary = Dictionary(vocab_file, min_cnt=0)\n",
        "    print(len(dictionary))\n",
        "    loaded = torch.load(model_file, map_location='cpu')\n",
        "    config = loaded['config']\n",
        "    gpt_config = config['embed_model_config']\n",
        "    gpt_config.attn_pdrop = 0\n",
        "    gpt_config.embd_pdrop = 0\n",
        "    gpt_config.resid_pdrop = 0\n",
        "    gpt_model = OpenAIGPTLMHeadModel(gpt_config)\n",
        "    model = GPTCoNuTModel(\n",
        "        dictionary=dictionary, embed_dim=config['embed_dim'],\n",
        "        max_positions=config['max_positions'],\n",
        "        src_encoder_convolutions=config['src_encoder_convolutions'],\n",
        "        ctx_encoder_convolutions=config['ctx_encoder_convolutions'],\n",
        "        decoder_convolutions=config['decoder_convolutions'],\n",
        "        dropout=0, embed_model=gpt_model,\n",
        "    )\n",
        "\n",
        "    model.load_state_dict(loaded['model'])\n",
        "    identifier_loader = IdentifierDataLoader(\n",
        "        dictionary, identifier_token_file, identifier_txt_file\n",
        "    )\n",
        "    data_loader = GPTCoNuTDataLoader(\n",
        "        input_file, dictionary,\n",
        "        identifier_loader=identifier_loader\n",
        "    )\n",
        "    generator = Generator(model, dictionary, data_loader, beam_size=beam_size)\n",
        "    print('start generate')\n",
        "    generator.generate(output_file)\n",
        "\n",
        "\n",
        "def generate_gpt_fconv(vocab_file, model_file, input_file, identifier_txt_file, identifier_token_file, output_file, beam_size):\n",
        "    dictionary = Dictionary(vocab_file, min_cnt=0)\n",
        "    print(len(dictionary))\n",
        "    loaded = torch.load(\n",
        "        model_file, map_location='cpu'\n",
        "    )\n",
        "    config = loaded['config']\n",
        "    gpt_config = config['embed_model_config']\n",
        "    gpt_config.attn_pdrop = 0\n",
        "    gpt_config.embd_pdrop = 0\n",
        "    gpt_config.resid_pdrop = 0\n",
        "    gpt_model = OpenAIGPTLMHeadModel(gpt_config)\n",
        "    model = GPTFConvModel(\n",
        "        dictionary=dictionary, embed_dim=config['embed_dim'],\n",
        "        max_positions=config['max_positions'],\n",
        "        encoder_convolutions=config['encoder_convolutions'],\n",
        "        decoder_convolutions=config['decoder_convolutions'],\n",
        "        dropout=0, embed_model=gpt_model,\n",
        "    )\n",
        "    model.load_state_dict(loaded['model'])\n",
        "    identifier_loader = IdentifierDataLoader(\n",
        "        dictionary, identifier_token_file, identifier_txt_file\n",
        "    )\n",
        "    data_loader = GPTFConvDataLoader(\n",
        "        input_file, dictionary,\n",
        "        identifier_loader=identifier_loader\n",
        "    )\n",
        "    generator = Generator(model, dictionary, data_loader, beam_size=beam_size)\n",
        "    print('start generate')\n",
        "    generator.generate(output_file)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    vocab_file = GENERATOR_DIR + '/data/vocabulary/vocabulary.txt'\n",
        "    input_file = GENERATOR_DIR + '/candidate_patches/QuixBugs/quixbugs_bpe.txt'\n",
        "    identifier_txt_file = GENERATOR_DIR + '/candidate_patches/QuixBugs/identifier.txt'\n",
        "    identifier_token_file = GENERATOR_DIR + '/candidate_patches/QuixBugs/identifier.tokens'\n",
        "    beam_size = 1000\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
        "\n",
        "    model_file = GENERATOR_DIR + '/data/models/gpt_conut_1.pt'\n",
        "    output_file = GENERATOR_DIR + '/data/patches/gpt_conut_1.txt'\n",
        "    generate_gpt_conut(vocab_file, model_file, input_file, identifier_txt_file, identifier_token_file, output_file, beam_size)\n",
        "\n",
        "    model_file = GENERATOR_DIR + '/data/models/gpt_fconv_1.pt'\n",
        "    output_file = GENERATOR_DIR + '/data/patches/gpt_fconv_1.txt'\n",
        "    generate_gpt_fconv(vocab_file, model_file, input_file, identifier_txt_file, identifier_token_file, output_file, beam_size)\n"
      ],
      "metadata": {
        "id": "ZaRpLCXstEU6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "853435ff-8c7d-43f3-f265-ea39b9e514fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8f61bc5934d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAIGPTLMHeadModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# GENERATOR_DIR = os.path.abspath(__file__)[: os.path.abspath(__file__).rindex('/') + 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}